{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment of an Intelligent Scenario based on HANA PAL via Python\n",
    "\n",
    "In the following notebook we will demonstrate how to work with the predictive analytics library (PAL) via the Python API and a development build of the [**hana_ml**](https://pypi.org/project/hana-ml/) package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contact Information\n",
    "\n",
    "- [Christoph Morgen, Product Manager](mailto:christoph.morgen@sap.com)\n",
    "- [Jonas Heinrich, VT Student](mailto:jonas.heinrich@sap.com)\n",
    "- [Florian Drescher, VT Student](mailto:florian.drescher@sap.com)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\"><b>Disclaimer</b></h3>\n",
    "\n",
    "This demo is executed on a live development environment currently in use by the ISLM team.\n",
    "Since there are several colleagues working on it frequently, **this demo does not claim stability**.\n",
    "\n",
    "**Additionally the part of this demo that focuses on the deployment is not released, nor is there any current release date targeted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario\n",
    "\n",
    "In this notebook, we use a common dataset that is about the onset of diabetes in the Indian population. It contains a table that has several columns as features, like age, bmi, insulin levels as well as the ultimate outcome (diabetes or not).\n",
    "\n",
    "We chose this for no particular reason, other than the dataset being in the [public domain](https://www.kaggle.com/uciml/pima-indians-diabetes-database), the prominence of it in the data science community and the ease of importing the data into an SAP System."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "\n",
    "This notebook is a mix of the typical workflow of a data scientist (with Python Code), as well as explanations and some screenshots from the Fiori frontend. You do not need to understand any Python code or the particular scenario we have chosen to follow this notebook.\n",
    "\n",
    "Without any further ado, lets get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "### 1. Install requirements\n",
    "\n",
    "Like any other machine learning library in the python ecosystem, we need to install the **hana_ml** package (a development build) in order to be able to import the necessary requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importing Requirements\n",
    "\n",
    "Let's import the necessary libraries for our use case. In here there is yaml for configuration management, a machine learning algorithm, a dataframe for data manipulation as well as the artifact generator and deployer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.algorithms.pal.unified_classification import UnifiedClassification\n",
    "import hana_ml.dataframe as dataframe\n",
    "\n",
    "from hana_ml.artifacts.generators import AMDPGenerator\n",
    "from hana_ml.artifacts.deployers import AMDPDeployer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create connection context\n",
    "\n",
    "In the following code block we just load our credentials from disk in order to not leak them into this notebook or the underlying git repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.algorithms.pal.utility import DataSets, Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import configparser\n",
    "except ImportError:\n",
    "    import ConfigParser as configparser\n",
    "Settings.settings = configparser.ConfigParser()\n",
    "Settings.settings.read(\"../../config/e2edata.ini\")\n",
    "url = Settings.settings.get(\"hana\", \"url\")\n",
    "port = Settings.settings.get(\"hana\", \"port\")\n",
    "user = Settings.settings.get(\"hana\", \"user\")\n",
    "passwd = Settings.settings.get(\"hana\", \"passwd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to create a connection context for our HANA system. This allows us to access the required data, as well as the PAL procedures we need to call in order to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_context = dataframe.ConnectionContext(\n",
    "    url, int(port), user, passwd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_context.hana_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_context.get_current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also enable SQL tracing for later reuse of the model in the deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare the data\n",
    "\n",
    "This block is part of the utils for this demo - it makes sure the dataset is in the system and creates it if necessary. In a real production use case this would obviously be unnecessary since the data is already in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_full, diabetes_train_valid, diabetes_test, _ = DataSets.load_diabetes_data(connection_context)\n",
    "\n",
    "diabetes_train_valid = diabetes_train_valid.save(\"DIABETES_TRAIN\", force=True)\n",
    "diabetes_test = diabetes_test.save(\"DIABETES_TEST\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Science Loop\n",
    "\n",
    "In this section the real work of a data scientist happens. They manipulate the data, preprocess columns, choose a model and try different combinations of hyper parameters.\n",
    "\n",
    "Since we just want to demonstrate the deployment, lets keep this short and just use a basic Random Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_context.sql_tracer.enable_sql_trace(True)\n",
    "connection_context.sql_tracer.enable_trace_history(True)\n",
    "rfc_params = dict(n_estimators=5, split_threshold=0, max_depth=10)\n",
    "rfc = UnifiedClassification(func=\"RandomDecisionTree\", **rfc_params)\n",
    "rfc.fit(diabetes_train_valid, \n",
    "        key='ID', \n",
    "        label='CLASS', \n",
    "        categorical_variable=['CLASS'],\n",
    "        partition_method='stratified',\n",
    "        stratified_column='CLASS',)\n",
    "cm = rfc.confusion_matrix_.collect()\n",
    "rfc.predict(diabetes_test.deselect('CLASS'), key=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the confusion matrix and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm)\n",
    "print(float(cm['COUNT'][cm['ACTUAL_CLASS'] == cm['PREDICTED_CLASS']].sum()) / cm['COUNT'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate abap managed database procedures (AMDP) artifact\n",
    "\n",
    "At this point in the workflow, our data scientist has iterated on the model many times and found a satisfactory solution. He/she now decides that its time to deploy this to an ABAP system such that an application developer can easily work with it.\n",
    "\n",
    "We start the process by creating some `.abap` files on our local machine based on the work that was done previously. This contains the SQL logic wrapped in AMDPs the data scientist created by interacting with the **hana_ml** package. You can also manually inspect the code at this point and make adaptions where you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = AMDPGenerator(project_name=\"PIMA_DIAB\", version=\"1\", connection_context=connection_context, outputdir=\"out/\")\n",
    "generator.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point in time there is no intelligent scenario present on the system:\n",
    "\n",
    "![pre.png](img/pre.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r out.zip out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Deploy the generated artifact to S/4\n",
    "\n",
    "We can now take the generated code and deploy it to S/4 or any ABAP stack with ISLM for that matter. We just need to provide the `.abap` file, and some basic parameters for the ISLM registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ZJMODEL_01\"\n",
    "model_description = \"ZJ Hello S/4 demo!\"\n",
    "scenario_name = \"ZJ_DEMO_CUSTOM01\"\n",
    "scenario_description = \"ZJ Hello S/4 Demo!\"\n",
    "scenario_type = \"CLASSIFICATION\"\n",
    "\n",
    "backend_url = Settings.settings.get(\"abap\", \"backend_url\")\n",
    "backend_user = Settings.settings.get(\"abap\", \"backend_user\")\n",
    "backend_password = Settings.settings.get(\"abap\", \"backend_password\")\n",
    "frontend_url = Settings.settings.get(\"abap\", \"frontend_url\")\n",
    "frontend_user = Settings.settings.get(\"abap\", \"frontend_user\")\n",
    "frontend_password = Settings.settings.get(\"abap\", \"frontend_password\")\n",
    "deployer = AMDPDeployer(backend_url=backend_url,\n",
    "                        backend_auth=(backend_user,\n",
    "                                      backend_password),\n",
    "                        frontend_url=frontend_url,\n",
    "                        frontend_auth=(frontend_user,\n",
    "                                       frontend_password))\n",
    "\n",
    "guid = deployer.deploy(fp=\"out/PIMADIAB/abap/Z_CL_PIMADIAB_1.abap\",\n",
    "                       model_name=model_name,\n",
    "                       catalog=\"$TMP\",\n",
    "                       scenario_name=scenario_name,\n",
    "                       scenario_description=scenario_description,\n",
    "                       scenario_type=scenario_type,\n",
    "                       force_overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now look into our fiori app, we can see that the scenario was created:\n",
    "\n",
    "![created.png](img/created.PNG)\n",
    "\n",
    "Looking into the scenario details, we can see that no CDS binding was supplied and it is currently kept as a draft:\n",
    "\n",
    "![created-details.png](img/created-details.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Publish the scenario to ISLM (optional)\n",
    "\n",
    "Altough this is not strictly necessary and should probably be done manually, we can also go ahead and publish the scenario with a training and apply dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployer.publish_islm(scenario_name, train_cds='ZPIMA_DIABETES_TRAIN', apply_cds='ZPIMA_DIABETES_TRAIN', sap_client='000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the Fiori app, we can now see that the scenario was published correctly.\n",
    "\n",
    "![published.png](img/published.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train the scenario in ISLM (optional)\n",
    "\n",
    "Taking this logic further, we can also start training a model programatically. Below screenshot is before the execution.\n",
    "\n",
    "![pretrain.png](img/pretrain.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployer.train_islm(model_name, model_description, scenario_name, sap_client='000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some minutes, the training was started and executed successfully.\n",
    "\n",
    "![posttrain.png](img/posttrain.png)\n",
    "\n",
    "We can also inspect model specific debriefing information:\n",
    "\n",
    "![debrief.png](img/debrief.png)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e8e26eb492012ce43ec3ea98c3fc2503d5495ecd40107dd94395e1e0d860e85"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
