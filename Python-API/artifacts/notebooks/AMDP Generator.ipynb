{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMDP Generator based on HANA PAL via Python\n",
    "\n",
    "In the following notebook we will demonstrate how to work with the predictive analytics library (PAL) via the Python API and a development build of the [**hana_ml**](https://pypi.org/project/hana-ml/) package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "### 1. Install requirements\n",
    "\n",
    "Like any other machine learning library in the python ecosystem, we need to install the **hana_ml** package (a development build) in order to be able to import the necessary requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importing Requirements\n",
    "\n",
    "Let's import the necessary libraries for our use case. In here there is yaml for configuration management, a machine learning algorithm, a dataframe for data manipulation as well as the artifact generator and deployer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.algorithms.pal.unified_classification import UnifiedClassification\n",
    "import hana_ml.dataframe as dataframe\n",
    "\n",
    "from hana_ml.artifacts.generators import AMDPGenerator\n",
    "from hana_ml.artifacts.deployers import AMDPDeployer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create connection context\n",
    "\n",
    "In the following code block we just load our credentials from disk in order to not leak them into this notebook or the underlying git repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .. import DataSets, Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import configparser\n",
    "except ImportError:\n",
    "    import ConfigParser as configparser\n",
    "Settings.settings = configparser.ConfigParser()\n",
    "Settings.settings.read(\"../../config/e2edata.ini\")\n",
    "url = Settings.settings.get(\"hana\", \"url\")\n",
    "port = Settings.settings.get(\"hana\", \"port\")\n",
    "user = Settings.settings.get(\"hana\", \"user\")\n",
    "passwd = Settings.settings.get(\"hana\", \"passwd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to create a connection context for our HANA system. This allows us to access the required data, as well as the PAL procedures we need to call in order to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_context = dataframe.ConnectionContext(\n",
    "    url, int(port), user, passwd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.50.000.00.1645273862 (master)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection_context.hana_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PAL_TEST'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection_context.get_current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also enable SQL tracing for later reuse of the model in the deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare the data\n",
    "\n",
    "This block is part of the utils for this demo - it makes sure the dataset is in the system and creates it if necessary. In a real production use case this would obviously be unnecessary since the data is already in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table PIMA_INDIANS_DIABETES_TBL exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hana_ml.ml_base:Executing SQL: DROP TABLE \"#PAL_PARTITION_DATA_TBL_D227A17F_B5AB_11EC_9E0F_F47B099F40D8\"\n",
      "INFO:hana_ml.ml_base:Executing SQL: CREATE LOCAL TEMPORARY COLUMN TABLE \"#PAL_PARTITION_DATA_TBL_D227A17F_B5AB_11EC_9E0F_F47B099F40D8\" AS (SELECT \"ID\", \"PREGNANCIES\", \"GLUCOSE\", \"SKINTHICKNESS\", \"INSULIN\", \"BMI\", \"AGE\", \"CLASS\" FROM (SELECT \"ID\", \"PREGNANCIES\", \"GLUCOSE\", \"SKINTHICKNESS\", \"INSULIN\", \"BMI\", \"AGE\", \"CLASS\" FROM (SELECT * FROM \"PAL_TEST\".\"PIMA_INDIANS_DIABETES_TBL\") AS \"DT_152\") AS \"DT_153\")\n",
      "INFO:hana_ml.ml_base:Executing SQL: DO\n",
      "BEGIN\n",
      "DECLARE param_name VARCHAR(5000) ARRAY;\n",
      "DECLARE int_value INTEGER ARRAY;\n",
      "DECLARE double_value DOUBLE ARRAY;\n",
      "DECLARE string_value VARCHAR(5000) ARRAY;\n",
      "param_name[1] := N'RANDOM_SEED';\n",
      "int_value[1] := 1234;\n",
      "double_value[1] := NULL;\n",
      "string_value[1] := NULL;\n",
      "param_name[2] := N'PARTITION_METHOD';\n",
      "int_value[2] := 0;\n",
      "double_value[2] := NULL;\n",
      "string_value[2] := NULL;\n",
      "param_name[3] := N'TRAINING_PERCENT';\n",
      "int_value[3] := NULL;\n",
      "double_value[3] := 0.5;\n",
      "string_value[3] := NULL;\n",
      "param_name[4] := N'TESTING_PERCENT';\n",
      "int_value[4] := NULL;\n",
      "double_value[4] := 0.1;\n",
      "string_value[4] := NULL;\n",
      "param_name[5] := N'VALIDATION_PERCENT';\n",
      "int_value[5] := NULL;\n",
      "double_value[5] := 0.4;\n",
      "string_value[5] := NULL;\n",
      "params = UNNEST(:param_name, :int_value, :double_value, :string_value);\n",
      "in_0 = SELECT \"ID\", \"PREGNANCIES\", \"GLUCOSE\", \"SKINTHICKNESS\", \"INSULIN\", \"BMI\", \"AGE\", \"CLASS\" FROM (SELECT \"ID\", \"PREGNANCIES\", \"GLUCOSE\", \"SKINTHICKNESS\", \"INSULIN\", \"BMI\", \"AGE\", \"CLASS\" FROM (SELECT * FROM \"PAL_TEST\".\"PIMA_INDIANS_DIABETES_TBL\") AS \"DT_152\") AS \"DT_153\";\n",
      "CALL _SYS_AFL.PAL_PARTITION(:in_0, :params, out_0);\n",
      "CREATE LOCAL TEMPORARY COLUMN TABLE \"#PAL_PARTITION_RESULT_TBL_D227A17F_B5AB_11EC_9E0F_F47B099F40D8\" AS (SELECT * FROM :out_0);\n",
      "END\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diabetes_full, diabetes_train_valid, diabetes_test, _ = DataSets.load_diabetes_data(connection_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hana_ml.ml_base:Executing SQL: DROP TABLE \"DIABETES_TRAIN\";\n",
      "INFO:hana_ml.ml_base:Executing SQL: CREATE COLUMN TABLE \"DIABETES_TRAIN\" AS (SELECT a.* FROM #PAL_PARTITION_DATA_TBL_D227A17F_B5AB_11EC_9E0F_F47B099F40D8 a inner join #PAL_PARTITION_RESULT_TBL_D227A17F_B5AB_11EC_9E0F_F47B099F40D8 b        on a.\"ID\" = b.\"ID\" where b.\"PARTITION_TYPE\" = 1)\n",
      "INFO:hana_ml.ml_base:Executing SQL: DROP TABLE \"DIABETES_TEST\";\n",
      "INFO:hana_ml.ml_base:Executing SQL: CREATE COLUMN TABLE \"DIABETES_TEST\" AS (SELECT a.* FROM #PAL_PARTITION_DATA_TBL_D227A17F_B5AB_11EC_9E0F_F47B099F40D8 a inner join #PAL_PARTITION_RESULT_TBL_D227A17F_B5AB_11EC_9E0F_F47B099F40D8 b        on a.\"ID\" = b.\"ID\" where b.\"PARTITION_TYPE\" = 3)\n"
     ]
    }
   ],
   "source": [
    "diabetes_train_valid = diabetes_train_valid.save(\"DIABETES_TRAIN\", force=True)\n",
    "diabetes_test = diabetes_test.save(\"DIABETES_TEST\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Science Loop\n",
    "\n",
    "In this section the real work of a data scientist happens. They manipulate the data, preprocess columns, choose a model and try different combinations of hyper parameters.\n",
    "\n",
    "Since we just want to demonstrate the deployment, lets keep this short and just use a basic Random Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hana_ml.ml_base:Executing SQL: DO\n",
      "BEGIN\n",
      "DECLARE param_name VARCHAR(5000) ARRAY;\n",
      "DECLARE int_value INTEGER ARRAY;\n",
      "DECLARE double_value DOUBLE ARRAY;\n",
      "DECLARE string_value VARCHAR(5000) ARRAY;\n",
      "param_name[1] := N'FUNCTION';\n",
      "int_value[1] := NULL;\n",
      "double_value[1] := NULL;\n",
      "string_value[1] := N'RDT';\n",
      "param_name[2] := N'KEY';\n",
      "int_value[2] := 1;\n",
      "double_value[2] := NULL;\n",
      "string_value[2] := NULL;\n",
      "param_name[3] := N'PARTITION_METHOD';\n",
      "int_value[3] := 2;\n",
      "double_value[3] := NULL;\n",
      "string_value[3] := NULL;\n",
      "param_name[4] := N'PARTITION_STRATIFIED_VARIABLE';\n",
      "int_value[4] := NULL;\n",
      "double_value[4] := NULL;\n",
      "string_value[4] := N'CLASS';\n",
      "param_name[5] := N'N_ESTIMATORS';\n",
      "int_value[5] := 5;\n",
      "double_value[5] := NULL;\n",
      "string_value[5] := NULL;\n",
      "param_name[6] := N'SPLIT_THRESHOLD';\n",
      "int_value[6] := NULL;\n",
      "double_value[6] := 0;\n",
      "string_value[6] := NULL;\n",
      "param_name[7] := N'MAX_DEPTH';\n",
      "int_value[7] := 10;\n",
      "double_value[7] := NULL;\n",
      "string_value[7] := NULL;\n",
      "param_name[8] := N'CATEGORICAL_VARIABLE';\n",
      "int_value[8] := NULL;\n",
      "double_value[8] := NULL;\n",
      "string_value[8] := N'CLASS';\n",
      "params = UNNEST(:param_name, :int_value, :double_value, :string_value);\n",
      "in_0 = SELECT \"ID\", \"PREGNANCIES\", \"GLUCOSE\", \"SKINTHICKNESS\", \"INSULIN\", \"BMI\", \"AGE\", \"CLASS\" FROM (SELECT * FROM \"DIABETES_TRAIN\") AS \"DT_160\";\n",
      "CALL _SYS_AFL.PAL_UNIFIED_CLASSIFICATION(:in_0, :params, out_0, out_1, out_2, out_3, out_4, out_5, out_6, out_7);\n",
      "CREATE LOCAL TEMPORARY COLUMN TABLE \"#PAL_UNIFIED_CLASSIFICATION_MODEL_5_DBD67AA8_B5AB_11EC_B2B7_F47B099F40D8\" AS (SELECT * FROM :out_0);\n",
      "CREATE LOCAL TEMPORARY COLUMN TABLE \"#PAL_UNIFIED_CLASSIFICATION_IMPORTANCE_5_DBD67AA8_B5AB_11EC_B2B7_F47B099F40D8\" AS (SELECT * FROM :out_1);\n",
      "CREATE LOCAL TEMPORARY COLUMN TABLE \"#PAL_UNIFIED_CLASSIFICATION_STATS_5_DBD67AA8_B5AB_11EC_B2B7_F47B099F40D8\" AS (SELECT * FROM :out_2);\n",
      "CREATE LOCAL TEMPORARY COLUMN TABLE \"#PAL_UNIFIED_CLASSIFICATION_OPT_PARAM_5_DBD67AA8_B5AB_11EC_B2B7_F47B099F40D8\" AS (SELECT * FROM :out_3);\n",
      "CREATE LOCAL TEMPORARY COLUMN TABLE \"#PAL_UNIFIED_CLASSIFICATION_CONFUSION_MATRIX_5_DBD67AA8_B5AB_11EC_B2B7_F47B099F40D8\" AS (SELECT * FROM :out_4);\n",
      "CREATE LOCAL TEMPORARY COLUMN TABLE \"#PAL_UNIFIED_CLASSIFICATION_METRICS_5_DBD67AA8_B5AB_11EC_B2B7_F47B099F40D8\" AS (SELECT * FROM :out_5);\n",
      "CREATE LOCAL TEMPORARY COLUMN TABLE \"#PAL_UNIFIED_CLASSIFICATION_PARTITION_TYPE_5_DBD67AA8_B5AB_11EC_B2B7_F47B099F40D8\" AS (SELECT * FROM :out_6);\n",
      "CREATE LOCAL TEMPORARY COLUMN TABLE \"#PAL_UNIFIED_CLASSIFICATION_PLACE_HOLDER2_5_DBD67AA8_B5AB_11EC_B2B7_F47B099F40D8\" AS (SELECT * FROM :out_7);\n",
      "END\n",
      "\n",
      "INFO:hana_ml.ml_base:Executing SQL: DO (IN in_1 TABLE (\"ROW_INDEX\" INT, \"PART_INDEX\" INT, \"MODEL_CONTENT\" NCLOB) => \"#PAL_UNIFIED_CLASSIFICATION_MODEL_5_DBD67AA8_B5AB_11EC_B2B7_F47B099F40D8\")\n",
      "BEGIN\n",
      "DECLARE param_name VARCHAR(5000) ARRAY;\n",
      "DECLARE int_value INTEGER ARRAY;\n",
      "DECLARE double_value DOUBLE ARRAY;\n",
      "DECLARE string_value VARCHAR(5000) ARRAY;\n",
      "param_name[1] := N'FUNCTION';\n",
      "int_value[1] := NULL;\n",
      "double_value[1] := NULL;\n",
      "string_value[1] := N'RDT';\n",
      "params = UNNEST(:param_name, :int_value, :double_value, :string_value);\n",
      "in_0 = SELECT \"ID\", \"PREGNANCIES\", \"GLUCOSE\", \"SKINTHICKNESS\", \"INSULIN\", \"BMI\", \"AGE\" FROM (SELECT \"ID\", \"PREGNANCIES\", \"GLUCOSE\", \"SKINTHICKNESS\", \"INSULIN\", \"BMI\", \"AGE\" FROM (SELECT * FROM \"DIABETES_TEST\") AS \"DT_164\") AS \"DT_181\";\n",
      "CALL _SYS_AFL.PAL_UNIFIED_CLASSIFICATION_PREDICT(:in_0, :in_1, :params, out_0, out_1);\n",
      "CREATE LOCAL TEMPORARY COLUMN TABLE \"#PAL_UNIFIED_CLASSIF_PREDICT_RESULT_TBL_5_E06757A3_B5AB_11EC_B34B_F47B099F40D8\" AS (SELECT * FROM :out_0);\n",
      "CREATE LOCAL TEMPORARY COLUMN TABLE \"#PAL_UNIFIED_CLASSIF_PREDICT_PH_TBL_5_E06757A3_B5AB_11EC_B34B_F47B099F40D8\" AS (SELECT * FROM :out_1);\n",
      "END\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hana_ml.dataframe.DataFrame at 0x1d8a30ad220>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection_context.sql_tracer.enable_sql_trace(True)\n",
    "connection_context.sql_tracer.enable_trace_history(True)\n",
    "rfc_params = dict(n_estimators=5, split_threshold=0, max_depth=10)\n",
    "rfc = UnifiedClassification(func=\"RandomDecisionTree\", **rfc_params)\n",
    "rfc.fit(diabetes_train_valid, \n",
    "        key='ID', \n",
    "        label='CLASS', \n",
    "        categorical_variable=['CLASS'],\n",
    "        partition_method='stratified',\n",
    "        stratified_column='CLASS',)\n",
    "cm = rfc.confusion_matrix_.collect()\n",
    "rfc.predict(diabetes_test.deselect(\"CLASS\"), key=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the confusion matrix and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACTUAL_CLASS PREDICTED_CLASS  COUNT\n",
      "0            0               0     39\n",
      "1            0               1     10\n",
      "2            1               0     17\n",
      "3            1               1     11\n",
      "0.6493506493506493\n"
     ]
    }
   ],
   "source": [
    "print(cm)\n",
    "print(float(cm['COUNT'][cm['ACTUAL_CLASS'] == cm['PREDICTED_CLASS']].sum()) / cm['COUNT'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate abap managed database procedures (AMDP) artifact\n",
    "\n",
    "At this point in the workflow, our data scientist has iterated on the model many times and found a satisfactory solution. He/she now decides that its time to deploy this to an ABAP system such that an application developer can easily work with it.\n",
    "\n",
    "We start the process by creating some `.abap` files on our local machine based on the work that was done previously. This contains the SQL logic wrapped in AMDPs the data scientist created by interacting with the **hana_ml** package. You can also manually inspect the code at this point and make adaptions where you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:hana_ml.artifacts.utils.fs_handler:[WinError 183] 当文件已存在时，无法创建该文件。: 'out\\\\AMDPDEMO\\\\abap'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generator = AMDPGenerator(project_name=\"AMDP_DEMO\", version=\"1\", connection_context=connection_context, outputdir=\"out\")\n",
    "generator.generate(training_dataset=\"DIABETES_TRAIN\", apply_dataset=\"DIABETES_TEST\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is based on a classification scenario template\n",
    "- Training and prediction datasets\n",
    "- Configurable PAL parameters\n",
    "- Training method includes\n",
    "   - data preprocessing\n",
    "   - partitioning\n",
    "   - model training\n",
    "   - scoring\n",
    "   - quality metrics and performance chart calculations\n",
    "- Predict method\n",
    "   - data preprocessing\n",
    "   - prediction call \n",
    "   - combining result set with reason codes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Generate AMDP artifact directly from unified classification object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.create_amdp_class(amdp_name=\"DIABETES_AMDP\", training_dataset=\"DIABETES_TRAIN\", apply_dataset=\"DIABETES_TEST\").build_amdp_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS DIABETES_AMDP DEFINITION\n",
      "  PUBLIC\n",
      "  FINAL\n",
      "  CREATE PUBLIC.\n",
      "\n",
      "  PUBLIC SECTION.\n",
      "    INTERFACES if_hemi_model_management.\n",
      "    INTERFACES if_hemi_procedure.\n",
      "    INTERFACES if_amdp_marker_hdb.\n",
      "\n",
      "    TYPES:\n",
      "      BEGIN OF ty_train_input,\n",
      "        id TYPE int4,\n",
      "        pregnancies TYPE int4,\n",
      "        glucose TYPE int4,\n",
      "        skinthickness TYPE int4,\n",
      "        insulin TYPE F,\n",
      "        bmi TYPE F,\n",
      "        age TYPE int4,\n",
      "        class TYPE int4,\n",
      "      END OF ty_train_input,\n",
      "      tt_training_data TYPE STANDARD TABLE OF ty_train_input WITH DEFAULT KEY,\n",
      "      tt_predict_data  TYPE STANDARD TABLE OF ty_train_input WITH DEFAULT KEY,\n",
      "      \n",
      "      BEGIN OF ty_predict_result,\n",
      "        id TYPE int4,\n",
      "        score TYPE int4,\n",
      "        confidence TYPE f,\n",
      "        reason_code_feature_1 TYPE shemi_reason_code_feature_name,\n",
      "        reason_code_percentage_1 TYPE shemi_reason_code_feature_pct,\n",
      "        reason_code_feature_2 TYPE shemi_reason_code_feature_name,\n",
      "        reason_code_percentage_2 TYPE shemi_reason_code_feature_pct,\n",
      "        reason_code_feature_3 TYPE shemi_reason_code_feature_name,\n",
      "        reason_code_percentage_3 TYPE shemi_reason_code_feature_pct,\n",
      "      END OF ty_predict_result,\n",
      "      tt_predict_result TYPE STANDARD TABLE OF ty_predict_result WITH DEFAULT KEY.\n",
      "\n",
      "    TYPES:\n",
      "      BEGIN OF ty_metrics,\n",
      "        key   TYPE string,\n",
      "        value TYPE string,\n",
      "      END OF ty_metrics,\n",
      "      tt_metrics TYPE STANDARD TABLE OF ty_metrics WITH DEFAULT KEY,\n",
      "      BEGIN OF ty_model,\n",
      "        row_index     TYPE int4,\n",
      "        part_index    TYPE int4,\n",
      "        model_content TYPE string,\n",
      "      END OF ty_model,\n",
      "      tt_model TYPE STANDARD TABLE OF ty_model WITH DEFAULT KEY.\n",
      "\n",
      "    CLASS-METHODS training\n",
      "      AMDP OPTIONS READ-ONLY\n",
      "      IMPORTING\n",
      "        VALUE(it_data)                TYPE tt_training_data\n",
      "        VALUE(it_param)               TYPE if_hemi_model_management=>tt_pal_param\n",
      "      EXPORTING\n",
      "        VALUE(et_model)               TYPE tt_model\n",
      "        VALUE(et_confusion_matrix)    TYPE shemi_confusion_matrix_t\n",
      "        VALUE(et_variable_importance) TYPE shemi_variable_importance_t\n",
      "        VALUE(et_metrics)             TYPE tt_metrics\n",
      "        VALUE(et_gen_info)            TYPE tt_metrics\n",
      "      RAISING\n",
      "        cx_amdp_execution_failed.\n",
      "\n",
      "    CLASS-METHODS predict_with_model_version\n",
      "      AMDP OPTIONS READ-ONLY\n",
      "      IMPORTING\n",
      "        VALUE(it_data)   TYPE tt_predict_data\n",
      "        VALUE(it_model)  TYPE tt_model\n",
      "        VALUE(it_param)  TYPE if_hemi_model_management=>tt_pal_param\n",
      "      EXPORTING\n",
      "        VALUE(et_result) TYPE tt_predict_result\n",
      "      RAISING\n",
      "        cx_amdp_execution_failed.\n",
      "\n",
      "  PROTECTED SECTION.\n",
      "  PRIVATE SECTION.\n",
      "ENDCLASS.\n",
      "\n",
      "CLASS DIABETES_AMDP IMPLEMENTATION.\n",
      "\n",
      "  METHOD if_hemi_model_management~get_amdp_class_name.\n",
      "    DATA lr_self TYPE REF TO DIABETES_AMDP.\n",
      "    TRY.\n",
      "        CREATE OBJECT lr_self.\n",
      "        ev_name = cl_abap_classdescr=>get_class_name( lr_self ).\n",
      "      CATCH cx_badi_context_error.\n",
      "      CATCH cx_badi_not_implemented.\n",
      "    ENDTRY.\n",
      "  ENDMETHOD.\n",
      "\n",
      "  METHOD if_hemi_procedure~get_procedure_parameters.\n",
      "    et_training = VALUE #(\n",
      "       ( name = 'IT_DATA'                role = cl_hemi_constants=>cs_proc_role-data                         )\n",
      "       ( name = 'IT_PARAM'               role = cl_hemi_constants=>cs_proc_role-param                        )\n",
      "       ( name = 'ET_MODEL'               role = cl_hemi_constants=>cs_proc_role-model                        )\n",
      "       ( name = 'ET_VARIABLE_IMPORTANCE' role = cl_hemi_constants=>cs_proc_role-stats add_info = 'imp'       )\n",
      "       ( name = 'ET_CONFUSION_MATRIX'    role = cl_hemi_constants=>cs_proc_role-stats add_info = 'confusion' )\n",
      "       ( name = 'ET_METRICS'             role = cl_hemi_constants=>cs_proc_role-stats add_info = 'metrics'   )\n",
      "       ( name = 'ET_GEN_INFO'            role = cl_hemi_constants=>cs_proc_role-stats add_info = 'gen'       )\n",
      "    ).\n",
      "    et_apply = VALUE #(\n",
      "       ( name = 'IT_DATA'   role = cl_hemi_constants=>cs_proc_role-data                        )\n",
      "       ( name = 'IT_MODEL'  role = cl_hemi_constants=>cs_proc_role-model add_info = 'et_model' )\n",
      "       ( name = 'IT_PARAM'  role = cl_hemi_constants=>cs_proc_role-param                       )\n",
      "       ( name = 'ET_RESULT' role = cl_hemi_constants=>cs_proc_role-result                      )\n",
      "    ).\n",
      "  ENDMETHOD.\n",
      "\n",
      "  METHOD if_hemi_model_management~get_meta_data.\n",
      "    es_meta_data-model_parameters = ( ( name = 'FUNCTION' type = cl_hemi_constants=>cs_param_type-string role = cl_hemi_constants=>cs_param_role-train configurable = abap_true has_context = abap_false )\n",
      "( name = 'KEY' type = cl_hemi_constants=>cs_param_type-string role = cl_hemi_constants=>cs_param_role-train configurable = abap_true has_context = abap_false )\n",
      "( name = 'PARTITION_METHOD' type = cl_hemi_constants=>cs_param_type-string role = cl_hemi_constants=>cs_param_role-train configurable = abap_true has_context = abap_false )\n",
      "( name = 'PARTITION_STRATIFIED_VARIABLE' type = cl_hemi_constants=>cs_param_type-string role = cl_hemi_constants=>cs_param_role-train configurable = abap_true has_context = abap_false )\n",
      "( name = 'N_ESTIMATORS' type = cl_hemi_constants=>cs_param_type-string role = cl_hemi_constants=>cs_param_role-train configurable = abap_true has_context = abap_false )\n",
      "( name = 'SPLIT_THRESHOLD' type = cl_hemi_constants=>cs_param_type-string role = cl_hemi_constants=>cs_param_role-train configurable = abap_true has_context = abap_false )\n",
      "( name = 'MAX_DEPTH' type = cl_hemi_constants=>cs_param_type-string role = cl_hemi_constants=>cs_param_role-train configurable = abap_true has_context = abap_false )\n",
      "( name = 'CATEGORICAL_VARIABLE' type = cl_hemi_constants=>cs_param_type-string role = cl_hemi_constants=>cs_param_role-train configurable = abap_true has_context = abap_false )\n",
      "name = 'FUNCTION' type = cl_hemi_constants=>cs_param_type-string role = cl_hemi_constants=>cs_param_role-apply configurable = abap_true has_context = abap_false ) ).\n",
      "    es_meta_data-model_parameter_defaults = ( ( name = 'FUNCTION' value = 'RDT' )\n",
      "( name = 'KEY' value = '1' )\n",
      "( name = 'PARTITION_METHOD' value = '2' )\n",
      "( name = 'PARTITION_STRATIFIED_VARIABLE' value = 'CLASS' )\n",
      "( name = 'N_ESTIMATORS' value = '5' )\n",
      "( name = 'SPLIT_THRESHOLD' value = '0.0' )\n",
      "( name = 'MAX_DEPTH' value = '10' )\n",
      "( name = 'CATEGORICAL_VARIABLE' value = 'CLASS' )\n",
      "( name = 'FUNCTION' value = 'RDT' ) ).\n",
      "\n",
      "    es_meta_data-training_data_set = 'DIABETES_TRAIN'.\n",
      "    es_meta_data-apply_data_set    = 'DIABETES_TEST'.\n",
      "\n",
      "    es_meta_data-field_descriptions = VALUE #( ( name = 'CLASS' role = cl_hemi_constants=>cs_field_role-target ) ).\n",
      "  ENDMETHOD.\n",
      "\n",
      "  METHOD training BY DATABASE PROCEDURE FOR HDB LANGUAGE SQLSCRIPT OPTIONS READ-ONLY.\n",
      "    declare complete_statistics table (stat_name nvarchar(256), stat_value nvarchar(1000), class_name nvarchar(256));\n",
      "    declare complete_metrics    table (metric_name nvarchar(256), x double, y double);\n",
      "    declare validation_ki double;\n",
      "    declare train_ki      double;\n",
      "\n",
      "    /* Step 1. Input data preprocessing (missing values, rescaling, encoding, etc)\n",
      "               Based on the scenario, add ids, select fields relevant for the training, cast to the appropriate data type, convert nulls into meaningful values.\n",
      "               Note: decimal must be converted into double.*/\n",
      "\n",
      "    call _sys_afl.pal_missing_value_handling(:it_data, :it_param, it_data, _);\n",
      "\n",
      "    /* Step 2. Sampling for training and model quality debriefing;\n",
      "               Imbalanced up/downsampling + Train/Test/Validation random or stratified partition sampling */\n",
      "\n",
      "    call _sys_afl.pal_partition(:it_data, :it_param, part_id);\n",
      "\n",
      "    lt_train = select lt.* from :it_data as lt, :part_id as part_id where lt.id = part_id.id and part_id.partition_type = 1;\n",
      "    lt_test  = select lt.* from :it_data as lt, :part_id as part_id where lt.id = part_id.id and part_id.partition_type = 2;\n",
      "    lt_val   = select lt.* from :it_data as lt, :part_id as part_id where lt.id = part_id.id and part_id.partition_type = 3;\n",
      "\n",
      "    /* Step 3. Unified classification training */\n",
      "\n",
      "    call _sys_afl.pal_unified_classification(:lt_train, :it_param, :et_model, :et_variable_importance, lt_stat, lt_opt, lt_cm, lt_metrics, lt_ph1, lt_ph2);\n",
      "\n",
      "    /* Step 4. Unified classification scoring + debriefing additional metrics and gain charts */\n",
      "\n",
      "    call _sys_afl.pal_unified_classification_score(:lt_train, :et_model, :it_param, result_train, stats_train, cm_train,  metrics_train);\n",
      "    call _sys_afl.pal_unified_classification_score(:lt_test,  :et_model, :it_param, result_test,  stats_test,  cm_test,   metrics_test);\n",
      "    call _sys_afl.pal_unified_classification_score(:lt_val,   :et_model, :it_param, result_val,   stats_val,   cm_val,    metrics_val);\n",
      "\n",
      "    -- output confusion matrix is derived from the validation dataset\n",
      "    et_confusion_matrix = select * from :cm_val;\n",
      "\n",
      "    complete_statistics = select concat('VALIDATION_', stat_name) as stat_name, stat_value, class_name from :stats_val\n",
      "                union all select concat('TEST_',       stat_name) as stat_name, stat_value, class_name from :stats_test\n",
      "                union all select concat('TRAIN_',      stat_name) as stat_name, stat_value, class_name from :stats_train;\n",
      "\n",
      "    complete_metrics = select concat('VALIDATION_', \"NAME\") as metric_name, x, y from :metrics_val\n",
      "             union all select concat('TEST_',       \"NAME\") as metric_name, x, y from :metrics_test\n",
      "             union all select concat('TRAIN_',      \"NAME\") as metric_name, x, y from :metrics_train;\n",
      "\n",
      "    -- Calculate KI and KR and other key metrics\n",
      "    select to_double(stat_value) * 2 - 1 into validation_ki from :complete_statistics where stat_name = 'VALIDATION_AUC';\n",
      "    select to_double(stat_value) * 2 - 1 into train_ki      from :complete_statistics where stat_name = 'TRAIN_AUC';\n",
      "\n",
      "    et_metrics = select 'PredictivePower'      as key, to_nvarchar(:validation_ki)                        as value from dummy\n",
      "       union all select 'PredictionConfidence' as key, to_nvarchar(1.0 - abs(:validation_ki - :train_ki)) as value from dummy\n",
      "    -- Provide metrics that are displayed in the quality information section of a model version in the ISLM Intelligent Scenario Management app\n",
      "    /* <<<<<< TODO: Starting point of adaptation */\n",
      "       union all select 'AUC'                  as key, stat_value                                         as value from :complete_statistics\n",
      "                    where stat_name = 'VALIDATION_AUC';\n",
      "    /* <<<<<< TODO: End point of adaptation */\n",
      "\n",
      "    gain_chart = select gerneral.X,\n",
      "                        (select min(y) from :complete_metrics as train_col      where metric_name = 'TRAIN_CUMGAINS'      and gerneral.x = train_col.x     ) as train,\n",
      "                        (select min(y) from :complete_metrics as validation_col where metric_name = 'VALIDATION_CUMGAINS' and gerneral.x = validation_col.x) as validation,\n",
      "                        (select min(y) from :complete_metrics as test_col       where metric_name = 'TEST_CUMGAINS'       and gerneral.x = test_col.x      ) as test,\n",
      "                        (select min(y) from :complete_metrics as wizard_col     where metric_name = 'TRAIN_PERF_CUMGAINS' and gerneral.x = wizard_col.x    ) as wizard\n",
      "                 from :complete_metrics as gerneral where gerneral.metric_name like_regexpr '(TRAIN|VALIDATION|TEST)_CUMGAINS' group by gerneral.x order by gerneral.x asc;\n",
      "\n",
      "    gain_chart = select t1.x, t1.train, t1.validation, t1.test, coalesce(t1.wizard, t2.wizard) as wizard from :gain_chart as t1\n",
      "        left outer join :gain_chart as t2 on t2.x = ( select max(t3.x) from :gain_chart as t3 where t3.x < t1.x and t3.wizard is not null);\n",
      "    gain_chart = select t1.x, coalesce(t1.train, t2.train) as train, t1.validation, t1.test, t1.wizard from :gain_chart as t1\n",
      "        left outer join :gain_chart as t2 on t2.x = ( select max(t3.x) from :gain_chart as t3 where t3.x < t1.x and t3.train is not null);\n",
      "    gain_chart = select t1.x, t1.train, coalesce(t1.validation, t2.validation) as validation, t1.test, t1.wizard from :gain_chart as t1\n",
      "        left outer join :gain_chart as t2 on t2.x = ( select max(t3.x) from :gain_chart as t3 where t3.x < t1.x and t3.validation is not null);\n",
      "    gain_chart = select t1.x, t1.train, t1.validation, coalesce(t1.test, t2.test) as test, t1.wizard from :gain_chart as t1\n",
      "        left outer join :gain_chart as t2 on t2.x = ( select max(t3.x) from :gain_chart as t3 where t3.x < t1.x and t3.TEST is not null) order by x;\n",
      "\n",
      "    et_gen_info = select 'HEMI_Profitcurve' as key,\n",
      "                         '{ \"Type\": \"detected\", \"Frequency\" : \"' || x || '\", \"Random\" : \"' || x || '\", \"Wizard\": \"' || wizard || '\", \"Estimation\": \"' || train || '\", \"Validation\": \"' || validation || '\", \"Test\": \"' || test || '\"}' as value\n",
      "                         from :gain_chart\n",
      "    -- Provide metrics that are displayed in the general additional info section of a model version in the ISLM Intelligent Scenario Management app\n",
      "    /* <<<<<< TODO: Starting point of adaptation */\n",
      "        union all select stat_name as key, stat_value as value from :complete_statistics where class_name is null;\n",
      "    /* <<<<<< TODO: End point of adaptation */\n",
      "  ENDMETHOD.\n",
      "\n",
      "  METHOD predict_with_model_version BY DATABASE PROCEDURE FOR HDB LANGUAGE SQLSCRIPT OPTIONS READ-ONLY.\n",
      "\n",
      "    /* Step 1. Input data preprocessing (missing values, rescaling, encoding, etc).\n",
      "               Note: the input data preprocessing must correspond with the one in the training method.\n",
      "               Based on the scenario, add ids, select fields relevant for the training, cast to the appropriate data type, convert nulls into meaningful values.\n",
      "               Note: decimal must be converted into double. */\n",
      "    lt_data = select\n",
      "                id,\n",
      "                pregnancies,\n",
      "                glucose,\n",
      "                skinthickness,\n",
      "                insulin,\n",
      "                bmi,\n",
      "                age\n",
      "              from :it_data;\n",
      "\n",
      "    call _sys_afl.pal_missing_value_handling(:lt_data, :it_param, lt_data_predict, lt_placeholder1);\n",
      "\n",
      "    /* Step 2. Execute prediction */\n",
      "\n",
      "    call _sys_afl.pal_unified_classification_predict(:lt_data_predict, :it_model, :it_param, lt_result, lt_placeholder2);\n",
      "\n",
      "    /* Step 3. Map prediction results back to the composite key */\n",
      "\n",
      "    et_result = select cast(result.ID as \"$ABAP.type( INT4 )\") as ID,\n",
      "                       cast(result.SCORE as \"$ABAP.type( INT4 )\") as SCORE,\n",
      "                       cast(result.CONFIDENCE as \"$ABAP.type( FLTP )\") as CONFIDENCE,\n",
      "                       trim(both '\"' from json_query(result.reason_code, '$[0].attr')) as reason_code_feature_1,\n",
      "                       json_query(result.reason_code, '$[0].pct' ) as reason_code_percentage_1,\n",
      "                       trim(both '\"' from json_query(result.reason_code, '$[1].attr')) as reason_code_feature_2,\n",
      "                       json_query(result.reason_code, '$[1].pct' ) as reason_code_percentage_2,\n",
      "                       trim(both '\"' from json_query(result.reason_code, '$[2].attr')) as reason_code_feature_3,\n",
      "                       json_query(result.reason_code, '$[2].pct' ) as reason_code_percentage_3\n",
      "                from :lt_result as result;\n",
      "  ENDMETHOD.\n",
      "\n",
      "ENDCLASS.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rfc.amdp_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.write_amdp_file(version=1, outdir='out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e8e26eb492012ce43ec3ea98c3fc2503d5495ecd40107dd94395e1e0d860e85"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
