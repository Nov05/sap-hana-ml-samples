{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Requirements\n",
    "\n",
    "Let's import the necessary libraries for our use case. In here there is yaml for configuration management, a machine learning algorithm, a dataframe for data manipulation as well as the artifact generator and deployer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.algorithms.pal.unified_classification import UnifiedClassification\n",
    "import hana_ml.dataframe as dataframe\n",
    "\n",
    "from hana_ml.artifacts.generators import hana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create connection context\n",
    "\n",
    "In the following code block we just load our credentials from disk in order to not leak them into this notebook or the underlying git repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.algorithms.pal.utility import DataSets, Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import configparser\n",
    "except ImportError:\n",
    "    import ConfigParser as configparser\n",
    "Settings.settings = configparser.ConfigParser()\n",
    "Settings.settings.read(\"../../config/e2edata.ini\")\n",
    "url = Settings.settings.get(\"hana\", \"url\")\n",
    "port = Settings.settings.get(\"hana\", \"port\")\n",
    "user = Settings.settings.get(\"hana\", \"user\")\n",
    "passwd = Settings.settings.get(\"hana\", \"passwd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to create a connection context for our HANA system. This allows us to access the required data, as well as the PAL procedures we need to call in order to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_context = dataframe.ConnectionContext(\n",
    "    url, int(port), user, passwd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_context.hana_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_context.get_current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also enable SQL tracing for later reuse of the model in the deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare the data\n",
    "\n",
    "This block is part of the utils for this demo - it makes sure the dataset is in the system and creates it if necessary. In a real production use case this would obviously be unnecessary since the data is already in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "diabetes_full, diabetes_train_valid, diabetes_test, _ = DataSets.load_diabetes_data(connection_context)\n",
    "diabetes_train_valid = diabetes_train_valid.save(\"diabetes_train_valid\")\n",
    "diabetes_test = diabetes_test.save(\"diabetes_test\")\n",
    "Settings.set_log_level()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_context.sql_tracer.enable_sql_trace(True)\n",
    "connection_context.sql_tracer.enable_trace_history(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Science Loop\n",
    "\n",
    "In this section the real work of a data scientist happens. They manipulate the data, preprocess columns, choose a model and try different combinations of hyper parameters.\n",
    "\n",
    "Since we just want to demonstrate the deployment, lets keep this short and just use a basic Random Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfc_params = dict(n_estimators=5, split_threshold=0, max_depth=10)\n",
    "rfc = UnifiedClassification(func=\"RandomDecisionTree\", **rfc_params)\n",
    "rfc.fit(diabetes_train_valid, \n",
    "        key='ID', \n",
    "        label='CLASS', \n",
    "        categorical_variable=['CLASS'],\n",
    "        partition_method='stratified',\n",
    "        stratified_column='CLASS',)\n",
    "cm = rfc.confusion_matrix_.collect()\n",
    "rfc.predict(diabetes_test.drop(cols=['CLASS']), key=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the confusion matrix and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm)\n",
    "print(float(cm['COUNT'][cm['ACTUAL_CLASS'] == cm['PREDICTED_CLASS']].sum()) / cm['COUNT'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate HDI artifact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg = hana.HanaGenerator(project_name=\"test\", version='1', grant_service='', connection_context=connection_context, outputdir=\"./hana_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg.config.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg.generate_artifacts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
