{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## **Association Analysis -  Sequential Pattern Mining (SPM)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Introduction and algorithm description\n",
    "- This notebook uses the real time itemset dataset to demonstrate the association rule mining algorithms below which are provided by the hana_ml.<br>\n",
    "<br>\n",
    "- **SPM(Sequential Pattern Mining)**\n",
    " The sequential pattern mining algorithm searches for frequent patterns in sequence databases. A sequence database consists of ordered elements or events. For example, a customer first buys bread, then eggs and cheese, and then milk. This forms a sequence consisting of three ordered events. We consider an event or a subsequent event is frequent if its support, which is the number of sequences that contain this event or subsequence, is greater than a certain value. This algorithm finds patterns in input sequences satisfying user defined minimum support."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Understand Sequence Pattern Mining before going into practice**<br>\n",
    "\n",
    "- T1: Find all subsets of items that occur with a specific sequence in all other transactions:\n",
    "      e.g {Playing cricket -> high ECG -> Sweating}\n",
    "- T2: Find all rules that correlate the order of one set of items after that another set of items in the transaction database:\n",
    "      e.g  72% of users who perform a web search then make a long eye gaze\n",
    "           over the ads follow that by a successful add-click \n",
    "**Prerequisites**<br>\n",
    "● The input data does not contain null value.<br> \n",
    "● There are no duplicated items in each transaction<br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "we will analyze the store data for frequent pattern mining ,this is the sample data which is available on SAP's help webpage."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **Attribute Information**<br>\n",
    " CUSTID -  Customer ID <br>\n",
    " TRANSID - Transaction ID <BR>\n",
    " ITEMS - Item of Transaction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Import Packages**\n",
    "First, import packages needed in the data loading."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml import dataframe\r\n",
    "from hana_ml.algorithms.pal.utility import Settings, DataSets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Setup Connection**\n",
    "In our case, the data is loaded into a table called \"PAL_APRIORI_TRANS_TBL\" in HANA from a csv file \"apriori_item_data.csv\". To do that, a connection to HANA is created and then passed to the data loader. To create a such connection, a config file, config/e2edata.ini is used to control the connection parameters. A sample section in the config file is shown below which includes HANA url, port, user and password information.<br>\n",
    "<br>\n",
    "###################<br>\n",
    "[hana]<br>\n",
    "url=host-url<br>\n",
    "user=username<br>\n",
    "passwd=userpassword<br>\n",
    "port=3xx15<br>\n",
    "<br>\n",
    "###################<br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "url, port, user, pwd = Settings.load_config(\"../../config/e2edata.ini\")\r\n",
    "# the connection\r\n",
    "#print(url , port , user , pwd)\r\n",
    "connection_context = dataframe.ConnectionContext(url, port, user, pwd)\r\n",
    "print(connection_context.connection.isconnected())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "  **Load Data**<br>\n",
    "   Then, the function DataSets.load_spm_data() is used to decide load or reload the data from scratch. If it is the first time to    load data, an exmaple of return message is shown below:\n",
    "   \n",
    "   ERROR:hana_ml.dataframe:Failed to get row count for the current Dataframe, (259, 'invalid table name:  Could not find table/view<BR> \n",
    "PAL_SPM_DATA_TBL in schema DM_PAL: line 1 col 37 (at pos 36)')<br>\n",
    "Table PAL_SPM_DATA_TBL doesn't exist in schema DM_PAL<br>\n",
    "Creating table PAL_SPM_DATA_TBL in schema DM_PAL ....<br>\n",
    "Drop unsuccessful<br>\n",
    "Creating table DM_PAL.PAL_SPM_DATA_TBL<br>\n",
    "Data Loaded:100%<br>\n",
    "   \n",
    "   #####################<br>\n",
    "   "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = DataSets.load_spm_data(connection_context)"
   ],
   "outputs": [],
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.collect().head(100) ##Display Data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df.dropna() ##Drop NAN if any of the blank record is present in your dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Toal Number of Records : \" + str(df.count()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Columns:\")\r\n",
    "df.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Filter**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.filter(\"CUSTID = 'A'\").head(10).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.filter('TRANSID = 1').head(100).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.filter(\"ITEMS = 'Apple'\").head(10).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Group by column**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.agg([('count' , 'ITEMS' , 'TOTAL TRANSACTIONS')] , group_by='ITEMS').head(100).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.agg([('count' , 'CUSTID', 'TOTAL TRANSACTIONS')] , group_by='CUSTID').head(100).collect()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.agg([('count' , 'TRANSID', 'TOTAL TRANSACTIONS')] , group_by='TRANSID').head(100).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Display the most popular items**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "from wordcloud import WordCloud\r\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\r\n",
    "wordcloud = WordCloud(background_color = 'white', width = 500,  height = 500, max_words = 120).generate(str(df_spm.head(100).collect()))\r\n",
    "plt.imshow(wordcloud)\r\n",
    "plt.axis('off')\r\n",
    "plt.title('Most Popular Items',fontsize = 10)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import SPM Method from HANA ML Library "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.filter(\"ITEMS = 'Blueberry'\").head(100).count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.algorithms.pal.association import SPM"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Setup SPM instance**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sp = SPM(min_support=0.5,\r\n",
    "         relational=False,\r\n",
    "         ubiquitous=1.0,\r\n",
    "         max_len=10,\r\n",
    "         min_len=1,\r\n",
    "         calc_lift=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sp.fit(data=df, customer='CUSTID', transaction='TRANSID', item='ITEMS')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Result Analysis**:<br>\n",
    "\n",
    "- Itemset Apple has support 1.0 indicates the frequencey of the item in all the transactions , most frequent item - confidence & lift is 0 for all the single items which states there is no antecedent & consequent item of them\n",
    "- Consider (Apple , Blueberry): Support is .88 (Frequeny of these items together is 88%) , Confidence is 88% means if someone is buying Apple then 88% chances they will also have blueberry in theri bucket , lif is .89 close to 1 indicates high Asscoiation of items\n",
    "- Benefit of having such kind of result is Storekeepers can easily look into purchasing Trends for their Shops\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sp.result_.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Attributes**\n",
    "\n",
    "- **result_**\n",
    "\n",
    "(DataFrame) The overall fequent pattern mining result, structured as follows: - 1st column : mined fequent patterns, - 2nd column : support values, - 3rd column : confidence values, - 4th column : lift values. Available only when relational is False.\n",
    "\n",
    "- **pattern_**\n",
    "\n",
    "(DataFrame) Result for mined requent patterns, structured as follows: - 1st column : pattern ID, - 2nd column : transaction ID, - 3rd column : items.\n",
    "\n",
    "- **stats_**\n",
    "\n",
    "(DataFrame) Statistics for frequent pattern mining, structured as follows: - 1st column : pattern ID, - 2nd column : support values, - 3rd column : confidence values, - 4th column : lift values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "connection_context.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "interpreter": {
   "hash": "9e8e26eb492012ce43ec3ea98c3fc2503d5495ecd40107dd94395e1e0d860e85"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}