{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DKOM 2019\n",
    "We show three areas of innovation.  The purpose is to demonstrate how data scientists can explore and perform machine learning tasks on large data sets that are stored in HANA.  We show the power of pushing processing power closer to where the data exists.  The benefits of using the power of HANA are:\n",
    "<li>Performance:  We see orders of magnitude performance gains and it only gets better when data sets are large.</li>\n",
    "<li>Security:  Since the data is all in HANA and processing done there, all the security measures are enforced along with the data.</li>\n",
    "<br>\n",
    "<br>\n",
    "From a data scientist point of view, they use Python and Python like APIs that they are comfortable with.\n",
    "<br>\n",
    "<br>\n",
    "We will cover the following:\n",
    "<li><b>Dataframes:</b>A reference to a relation in HANA.  No need for deep SQL knowledge.</li>\n",
    "<li><b>HANA ML API:</b>Exploit HANA's ML capabilities using a SciKit type of Python interface.</li>\n",
    "<li><b>Exploratory Data Analysis and Visualization:</b>Analyze large data sets without the performance penalty or running out of resources on the client</li>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataframes\n",
    "The SAP HANA Python Client API for machine learning algorithms (Python Client API for ML) provides a set of client-side Python functions for accessing and querying SAP HANA data, and a set of functions for developing machine learning models.\n",
    "\n",
    "The Python Client API for ML consists of two main parts:\n",
    "\n",
    "<li>A set of machine learning APIs for different algorithms.</li>\n",
    "<li>The SAP HANA dataframe, which provides a set of methods for analyzing data in SAP HANA without bringing that data to the client.</li>\n",
    "\n",
    "This library uses the SAP HANA Python driver (hdbcli) to connect to and access SAP HANA.\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"images/highlevel_overview2_new.png\" title=\"Python API Overview\" style=\"float:left;\" width=\"300\" height=\"50\" />\n",
    "<br>\n",
    "A dataframe represents a table (or any SQL statement).  Most operations on a dataframe are designed to not bring data back from the database unless explicitly asked for."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml import dataframe\r\n",
    "import logging"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup connection and data sets\n",
    "Let us load some data into a HANA table.  The data is loaded into 4 tables - full set, test set, training set, and the validation set:DBM2_RFULL_TBL, DBM2_RTEST_TBL, DBM2_RTRAINING_TBL, DBM2_RVALIDATION_TBL.\n",
    "\n",
    "The data is related with direct marketing campaigns of a Portuguese banking institution. More information regarding the data set is at https://archive.ics.uci.edu/ml/datasets/bank+marketing#. For tutorials use only.\n",
    "\n",
    "To do that, a connection is created and passed to the loader.  There is a config file, <b>config/e2edata.ini</b> that controls the connection parameters.  Please edit it to point to your hana instance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.algorithms.pal.utility import DataSets, Settings\r\n",
    "url, port, user, pwd = Settings.load_config(\"../../config/e2edata.ini\")\r\n",
    "connection_context = dataframe.ConnectionContext(url, port, user, pwd)\r\n",
    "full_set, training_set, validation_set, test_set = DataSets.load_bank_data(connection_context, force=False, chunk_size=50000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple DataFrame\n",
    "A dataframe is a reference to a relation.  This can be a table, view, or any relation from a SQL statement\n",
    "<table align=\"left\"><tr><td>\n",
    "</td><td><img src=\"images/Dataframes_1.png\" style=\"float:left;\" width=\"600\" height=\"400\" /></td></tr></table>\n",
    "<br>\n",
    "<b>Let's take a look at a dataframe created using our training table.</b>\n",
    "<br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = training_set\r\n",
    "print(dataset.select_statement)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(type(dataset))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bring data to client\n",
    "#### Fetch 5 rows into client as a <b>Pandas Dataframe</b>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset.head(5).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SQL Operations\n",
    "We now show simple SQL operations.  No extensive SQL knowledge is needed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Projection\n",
    "<img src=\"images/Projection.png\" style=\"float:left;\" width=\"150\" height=\"750\" />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dsp = dataset.select(\"ID\", \"AGE\", \"JOB\", ('\"AGE\"*2', \"TWICE_AGE\"))\r\n",
    "dsp.head(5).collect()  # collect() brings data to the client)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filtering Data\n",
    "<img src=\"images/Filter.png\" style=\"float:left;\" width=\"200\" height=\"100\" />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset.filter('AGE > 60').head(10).collect()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sorting\n",
    "<img src=\"images/Sort.png\" style=\"float:left;\" width=\"200\" height=\"100\" />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset.filter('AGE>60').sort(['AGE']).head(2).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grouping Data\n",
    "<img src=\"images/Grouping.png\" style=\"float:left;\" width=\"300\" height=\"200\" />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset.agg([('count', 'AGE', 'COUNT_OF_AGE')], group_by='AGE').head(4).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple Joins\n",
    "<img src=\"images/Join.png\" style=\"float:left;\" width=\"300\" height=\"200\" />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds1 = dataset.select([\"ID\", \"AGE\"])\r\n",
    "ds2 = dataset.select([\"ID\", \"JOB\"])\r\n",
    "condition = '{}.\"ID\"={}.\"ID\"'.format(ds1.quoted_name, ds2.quoted_name)\r\n",
    "dsj = ds1.join(ds2, condition)\r\n",
    "dsj.select_statement"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Describing a dataframe\n",
    "<img src=\"images/Describe.png\" style=\"float:left;\" width=\"300\" height=\"200\" />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset.describe().collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset.describe().select_statement"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ML API Wrapping Predictive Analytics Library"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification - Logistic Regression Example\n",
    "### Bank dataset to determine if a customer would buy a CD\n",
    "The data is related with direct marketing campaigns of a Portuguese banking institution.  The marketing campaigns were based on phone calls.  A number of features such as age, kind of job, marital status, education level, credit default, existence of housing loan, etc. were considered.  The classification goal is to predict if the client will subscribe (yes/no) a term deposit.\n",
    "\n",
    "More information regarding the data set is at https://archive.ics.uci.edu/ml/datasets/bank+marketing#. For tutorials use only.\n",
    "\n",
    "<font color='blue'>__ The objective is to demonstrate the use of logistic regression and to tune hyperparameters enet_lamba and enet_alpha. __</font>\n",
    "\n",
    "### Attribute Information:\n",
    "\n",
    "#### Input variables:\n",
    "##### Bank client data:\n",
    "1. age (numeric)\n",
    "2. job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "3. marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4. education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "5. default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6. housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "7. loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "\n",
    "##### Related with the last contact of the current campaign:\n",
    "8. contact: contact communication type (categorical: 'cellular','telephone') \n",
    "9. month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "10. day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "11. duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "##### Other attributes:\n",
    "12. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14. previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "15. poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "\n",
    "##### Social and economic context attributes:\n",
    "16. emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "17. cons.price.idx: consumer price index - monthly indicator (numeric) \n",
    "18. cons.conf.idx: consumer confidence index - monthly indicator (numeric) \n",
    "19. euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "20. nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "#### Output variable (desired target):\n",
    "21. y - has the client subscribed a term deposit? (binary: 'yes','no')\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the data set and create data frames"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml import dataframe\r\n",
    "from hana_ml.algorithms.pal import linear_model\r\n",
    "from hana_ml.algorithms.pal import clustering\r\n",
    "from hana_ml.algorithms.pal import trees\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import logging\r\n",
    "from IPython.core.display import Image, display"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.algorithms.pal.utility import DataSets, Settings\r\n",
    "url, port, user, pwd = Settings.load_config(\"../../config/e2edata.ini\")\r\n",
    "connection_context = dataframe.ConnectionContext(url, port, user, pwd)\r\n",
    "full_set, training_set, validation_set, test_set = DataSets.load_bank_data(connection_context, force=False, chunk_size=50000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let us look at some rows"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_set.head(5).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Model and Tune Hyperparameters\n",
    "Try different hyperparameters and see what parameter is best.\n",
    "The results are stored in a list called res which can then be used to visualize the results.\n",
    "\n",
    "_The variable \"quick\" is to run the tests for only a few values to avoid running the code below for a long time._\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features = ['AGE','JOB','MARITAL','EDUCATION','DBM_DEFAULT', 'HOUSING','LOAN','CONTACT','DBM_MONTH','DAY_OF_WEEK','DURATION','CAMPAIGN','PDAYS','PREVIOUS','POUTCOME','EMP_VAR_RATE','CONS_PRICE_IDX','CONS_CONF_IDX','EURIBOR3M','NREMPLOYED']\r\n",
    "label = \"LABEL\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "quick = True\r\n",
    "enet_lambdas = np.linspace(0.01,0.02, endpoint=False, num=1) if quick else np.append(np.linspace(0.01,0.02, endpoint=False, num=4), np.linspace(0.02,0.02, num=5))\r\n",
    "enet_alphas = np.linspace(0, 1, num=4) if quick else np.linspace(0, 1, num=40)\r\n",
    "res = []\r\n",
    "for enet_alpha in enet_alphas:\r\n",
    "    for enet_lambda in enet_lambdas:\r\n",
    "        lr = linear_model.LogisticRegression(solver='Cyclical', tol=0.000001, max_iter=10000, \r\n",
    "                                             stat_inf=True,pmml_export='multi-row', enet_lambda=enet_lambda, enet_alpha=enet_alpha,\r\n",
    "                                             class_map0='no', class_map1='yes')\r\n",
    "        lr.fit(training_set, features=features, label=label)\r\n",
    "        accuracy_val = lr.score(validation_set, 'ID', features, label)\r\n",
    "        res.append((enet_alpha, enet_lambda, accuracy_val, lr.coef_))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graph the results\n",
    "Plot the accuracy on the validation set against the hyperparameters.\n",
    "\n",
    "This is only done if all the combinations are tried."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\r\n",
    "if not quick:\r\n",
    "    arry = np.asarray(res)\r\n",
    "    fig = plt.figure(figsize=(10,10))\r\n",
    "    plt.title(\"Validation accuracy for training set with different lambdas\")\r\n",
    "    ax = fig.add_subplot(111)\r\n",
    "    most_accurate_lambda = arry[np.argmax(arry[:,2]),1]\r\n",
    "    best_accuracy_arg = np.argmax(arry[:,2])\r\n",
    "    for lamda in enet_lambdas:\r\n",
    "        if lamda == most_accurate_lambda:\r\n",
    "            ax.plot(arry[arry[:,1]==lamda][:,0], arry[arry[:,1]==lamda][:,2], label=\"%.3f\" % round(lamda,3), linewidth=5, c='r')\r\n",
    "        else:\r\n",
    "            ax.plot(arry[arry[:,1]==lamda][:,0], arry[arry[:,1]==lamda][:,2], label=\"%.3f\" % round(lamda,3))\r\n",
    "    plt.legend(loc=1, title=\"Legend (Lambda)\", fancybox=True, fontsize=12)\r\n",
    "    ax.set_xlabel('Alpha', fontsize=12)\r\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\r\n",
    "    plt.xticks(fontsize=12)\r\n",
    "    plt.yticks(fontsize=12)\r\n",
    "    plt.grid()\r\n",
    "    plt.show()\r\n",
    "    print(\"Best accuracy: %.4f\" % (arry[best_accuracy_arg][2]))\r\n",
    "    print(\"Value of alpha for maximum accuracy: %.3f\\nValue of lambda for maximum accuracy: %.3f\\n\" % (arry[best_accuracy_arg][0], arry[best_accuracy_arg][1]))\r\n",
    "else:\r\n",
    "    display(Image('images/bank-data-hyperparameter-tuning.png', width=800, unconfined=True))\r\n",
    "    print(\"Best accuracy: 0.9148\")\r\n",
    "    print(\"Value of alpha for maximum accuracy: 0.769\")\r\n",
    "    print(\"Value of lambda for maximum accuracy: 0.010\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predictions on test set\n",
    "Let us do the predictions on the test set using these values of alpha and lambda"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "alpha = 0.769\r\n",
    "lamda = 0.01\r\n",
    "lr = linear_model.LogisticRegression(solver='Cyclical', tol=0.000001, max_iter=10000, \r\n",
    "                                       stat_inf=True,pmml_export='multi-row', enet_lambda=lamda, enet_alpha=alpha,\r\n",
    "                                       class_map0='no', class_map1='yes')\r\n",
    "lr.fit(training_set, features=features, label=label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Look at the predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result_df = lr.predict(test_set, 'ID')\r\n",
    "result_df.filter('\"CLASS\"=\\'no\\'').head(5).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What about the final score?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lr.score(test_set, 'ID')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KMeans Clustering Example"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A data set that identifies different types of iris's is used to demonstrate KMeans in SAP HANA.\n",
    "## Iris Data Set\n",
    "The data set used is from University of California, Irvine (https://archive.ics.uci.edu/ml/datasets/iris). For tutorials use only. This data set contains attributes of a plant iris.  There are three species of Iris plants.\n",
    "<table>\n",
    "<tr><td>Iris Setosa</td><td><img src=\"images/Iris_setosa.jpg\" title=\"Iris Sertosa\" style=\"float:left;\" width=\"300\" height=\"50\" /></td>\n",
    "<td>Iris Versicolor</td><td><img src=\"images/Iris_versicolor.jpg\" title=\"Iris Versicolor\" style=\"float:left;\" width=\"300\" height=\"50\" /></td>\n",
    "<td>Iris Virginica</td><td><img src=\"images/Iris_virginica.jpg\" title=\"Iris Virginica\" style=\"float:left;\" width=\"300\" height=\"50\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "The data contains the following attributes for various flowers:\n",
    "<table align=\"left\"><tr><td>\n",
    "<li align=\"top\">sepal length in cm</li>\n",
    "<li align=\"left\">sepal width in cm</li>\n",
    "<li align=\"left\">petal length in cm</li>\n",
    "<li align=\"left\">petal width in cm</li>\n",
    "</td><td><img src=\"images/sepal_petal.jpg\" style=\"float:left;\" width=\"200\" height=\"40\" /></td></tr></table>\n",
    "\n",
    "Although the flower is identified in the data set, we will cluster the data set into 3 clusters since we know there are three different flowers.  The hope is that the cluster will correspond to each of the flowers.\n",
    "\n",
    "A different notebook will use a classification algorithm to predict the type of flower based on the sepal and petal dimensions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the data set and create data frames"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml import dataframe\r\n",
    "from hana_ml.algorithms.pal import clustering\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import logging\r\n",
    "import itertools\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.algorithms.pal.utility import DataSets, Settings\r\n",
    "url, port, user, pwd = Settings.load_config(\"../../config/e2edata.ini\")\r\n",
    "connection_context = dataframe.ConnectionContext(url, port, user, pwd)\r\n",
    "full_set, training_set, validation_set, test_set = DataSets.load_iris_data(connection_context, force=False, chunk_size=50000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's check how many SPECIES are in the data set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "full_set.distinct(\"SPECIES\").collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Model\n",
    "The lines below show the ease with which clustering can be done."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set up the features and labels for the model and create the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features = ['SEPALLENGTHCM','SEPALWIDTHCM','PETALLENGTHCM','PETALWIDTHCM']\r\n",
    "label = ['SPECIES']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kmeans = clustering.KMeans(thread_ratio=0.2, n_clusters=3, distance_level='euclidean', \r\n",
    "                           max_iter=100, tol=1.0E-6, category_weights=0.5, normalization='min_max')\r\n",
    "predictions = kmeans.fit_predict(full_set, 'ID', features).collect()\r\n",
    "predictions.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_kmeans_results(data_set, features, predictions):\r\n",
    "    # use this to estimate what each cluster_id represents in terms of flowers\r\n",
    "    # ideal would be 50-50-50 for each flower, so we can see there are some mis clusterings\r\n",
    "    class_colors = {0: 'r', 1: 'b', 2: 'k'}\r\n",
    "    predictions_colors = [class_colors[p] for p in predictions['CLUSTER_ID'].values]\r\n",
    "\r\n",
    "    red = plt.Line2D(range(1), range(1), c='w', marker='o', markerfacecolor='r', label='Iris-virginica', markersize=10, alpha=0.9)\r\n",
    "    blue = plt.Line2D(range(1), range(1), c='w', marker='o', markerfacecolor='b', label='Iris-versicolor', markersize=10, alpha=0.9)\r\n",
    "    black = plt.Line2D(range(1), range(1), c='w', marker='o', markerfacecolor='k', label='Iris-setosa', markersize=10, alpha=0.9)\r\n",
    "\r\n",
    "    for x, y in itertools.combinations(features, 2):\r\n",
    "        plt.figure(figsize=(10,5))\r\n",
    "        plt.scatter(full_set[[x]].collect(), data_set[[y]].collect(), c=predictions_colors, alpha=0.6, s=70)\r\n",
    "        plt.grid()\r\n",
    "        plt.xlabel(x, fontsize=15)\r\n",
    "        plt.ylabel(y, fontsize=15)\r\n",
    "        plt.tick_params(labelsize=15)\r\n",
    "        plt.legend(handles=[red, blue, black])\r\n",
    "        plt.show()\r\n",
    "\r\n",
    "    %matplotlib inline\r\n",
    "    #above allows interactive 3d plot\r\n",
    "\r\n",
    "    sizes=10\r\n",
    "    for x, y, z in itertools.combinations(features, 3):\r\n",
    "        fig = plt.figure(figsize=(8,5))\r\n",
    "\r\n",
    "        ax = fig.add_subplot(111, projection='3d')\r\n",
    "        ax.scatter3D(data_set[[x]].collect(), data_set[[y]].collect(), data_set[[z]].collect(), c=predictions_colors, s=70)\r\n",
    "        plt.grid()\r\n",
    "\r\n",
    "        ax.set_xlabel(x, labelpad=sizes, fontsize=sizes)\r\n",
    "        ax.set_ylabel(y, labelpad=sizes, fontsize=sizes)\r\n",
    "        ax.set_zlabel(z, labelpad=sizes, fontsize=sizes)\r\n",
    "        ax.tick_params(labelsize=sizes)\r\n",
    "        plt.legend(handles=[red, blue, black])\r\n",
    "        plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(pd.concat([predictions, full_set[['SPECIES']].collect()], axis=1).groupby(['SPECIES','CLUSTER_ID']).size())\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\r\n",
    "plot_kmeans_results(full_set, features, predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis and Visualization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Titanic Data Set (~1K rows)\n",
    "This dataset is from https://github.com/awesomedata/awesome-public-datasets/tree/master/Datasets For tutorials use only."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml import dataframe\r\n",
    "from hana_ml.algorithms.pal import trees\r\n",
    "from hana_ml.visualizers.eda import EDAVisualizer as eda\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import time\r\n",
    "from hana_ml.visualizers.eda import EDAVisualizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.algorithms.pal.utility import DataSets, Settings\r\n",
    "url, port, user, pwd = Settings.load_config(\"../../config/e2edata.ini\")\r\n",
    "connection_context = dataframe.ConnectionContext(url, port, user, pwd)\r\n",
    "full_set, training_set, validation_set, test_set = DataSets.load_titanic_data(connection_context, force=True, chunk_size=50000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create the HANA Dataframe (df_train) and point to the training table.\r\n",
    "data = full_set\r\n",
    "data = data.fillna(25, ['AGE'])\r\n",
    "data.head(5).collect()\r\n",
    "data.dtypes()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Histogram plot for AGE distribution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bins=15\r\n",
    "f = plt.figure(figsize=(bins*1.5, bins*0.5))\r\n",
    "ax1 = f.add_subplot(121)\r\n",
    "ax2 = f.add_subplot(122)\r\n",
    "start = time.time()\r\n",
    "eda = EDAVisualizer(ax1)\r\n",
    "ax1, dist_data1 = eda.distribution_plot(data, column=\"AGE\", bins=bins, title=\"Distribution of AGE (All)\")\r\n",
    "eda = EDAVisualizer(ax2)\r\n",
    "ax2, dist_data2 = eda.distribution_plot(data.filter('SURVIVED=1'), column=\"AGE\", bins=bins, title=\"Distribution of AGE (Survived)\")\r\n",
    "end = time.time()\r\n",
    "plt.show()\r\n",
    "print(\"Time: {}s.  Time taken to do this by getting the data from the server was 0.86s\".format(round(end-start, 2)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Pie plot for PCLASS (passenger class) distribution"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f = plt.figure(figsize=(20,10))\r\n",
    "ax1 = f.add_subplot(121)\r\n",
    "ax2 = f.add_subplot(122)\r\n",
    "start = time.time()\r\n",
    "eda = EDAVisualizer(ax1)\r\n",
    "ax1, pie_data = eda.pie_plot(data, column=\"PCLASS\", title=\"Proportion of passengers in each class\")\r\n",
    "eda = EDAVisualizer(ax2)\r\n",
    "ax2, pie_data = eda.pie_plot(data.filter('SURVIVED=1'), column=\"PCLASS\", title=\"Proportion of passengers in each class who survived\")\r\n",
    "end = time.time()\r\n",
    "plt.show()\r\n",
    "print(\"Time: {}s.  Time taken to do this by getting the data from the server was 0.88s\".format(round(end-start, 2)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correlation plot - Look at all numeric columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f = plt.figure(figsize=(10,10))\r\n",
    "ax1 = f.add_subplot(111)\r\n",
    "start = time.time()\r\n",
    "eda = EDAVisualizer(ax1)\r\n",
    "ax1, corr = eda.correlation_plot(data)\r\n",
    "end = time.time()\r\n",
    "plt.show()\r\n",
    "print(\"Time: {}s.  Time taken to do this by getting the data from the server was 2s\".format(round(end-start, 2)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Performance Comparison"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Box plot time for the large data set is 1600s!\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "col_names =  ['chart', 'dataSize', 'noDataframe', 'withDataframe']\r\n",
    "comparison_df  = pd.DataFrame(columns = col_names)\r\n",
    "comparison_df.loc[len(comparison_df)] = ['Distribution', '1K',  0.86,  0.14]\r\n",
    "comparison_df.loc[len(comparison_df)] = ['Pie', '1K',  0.88,  0.09]\r\n",
    "comparison_df.loc[len(comparison_df)] = ['Correlation', '1K',  2.0,   2.78]\r\n",
    "\r\n",
    "comparison_df.loc[len(comparison_df)] = ['Distribution', '8K',  7.5,   0.18]\r\n",
    "comparison_df.loc[len(comparison_df)] = ['Pie', '8K',  7.6,   0.26]\r\n",
    "comparison_df.loc[len(comparison_df)] = ['Correlation', '8K',  9.2,   2.1]\r\n",
    "\r\n",
    "comparison_df.loc[len(comparison_df)] = ['Distribution', '500K',  360,   0.29]\r\n",
    "comparison_df.loc[len(comparison_df)] = ['Pie', '500K',  450,   0.23]\r\n",
    "comparison_df.loc[len(comparison_df)] = ['Correlation', '500K',  400,   4.3]\r\n",
    "\r\n",
    "comparison_df.loc[len(comparison_df)] = ['Distribution', '1M',  950,   0.33]\r\n",
    "comparison_df.loc[len(comparison_df)] = ['Pie', '1M',  940,   0.22]\r\n",
    "comparison_df.loc[len(comparison_df)] = ['Correlation', '1M',  950,   6.28]\r\n",
    "comparison_df['noDataframe'] = np.log10(comparison_df['noDataframe']*1000)\r\n",
    "comparison_df['withDataframe'] = np.log10(comparison_df['withDataframe']*1000)\r\n",
    "#comparison_df[comparison_df['chart'] == 'Distribution']['noDataframe']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f = plt.figure(figsize=(15,10))\r\n",
    "ax = f.add_subplot(111)\r\n",
    "N = 4\r\n",
    "width = 0.10\r\n",
    "ind = np.arange(N) \r\n",
    "ax.bar(ind,           comparison_df[comparison_df['chart'] == 'Distribution']['noDataframe'], width, label='Distribution (No DF)')\r\n",
    "ax.bar(ind +   width, comparison_df[comparison_df['chart'] == 'Distribution']['withDataframe'], width, label='Distribution (DF)')\r\n",
    "gap = 0.05\r\n",
    "ax.bar(ind + gap + 2*width, comparison_df[comparison_df['chart'] == 'Pie']['noDataframe'], width, label='Pie (No DF)')\r\n",
    "ax.bar(ind + gap + 3*width, comparison_df[comparison_df['chart'] == 'Pie']['withDataframe'], width, label='Pie (DF)')\r\n",
    "\r\n",
    "ax.bar(ind + 2*gap + 4*width, comparison_df[comparison_df['chart'] == 'Correlation']['noDataframe'], width, label='Correlation (No DF)')\r\n",
    "ax.bar(ind + 2*gap + 5*width, comparison_df[comparison_df['chart'] == 'Correlation']['withDataframe'], width, label='Correlation (DF)')\r\n",
    "\r\n",
    "\r\n",
    "#plt.ylabel('Scores')\r\n",
    "#plt.title('Scores by group and gender')\r\n",
    "\r\n",
    "#ax.xticks(ind + width*2, comparison_df['dataSize'].unique())\r\n",
    "ax.set_xticks(ind + width*2)\r\n",
    "ax.set_xticklabels(comparison_df['dataSize'].unique(), fontsize=20)\r\n",
    "ax.set_xlabel(\"Data Size (# Rows)\", fontsize=20)\r\n",
    "ax.set_yticklabels([0, .01, .1, 1, 10, 100, 1000], fontsize=20)\r\n",
    "ax.set_ylabel(\"Time in seconds (Log Scale)\", fontsize=20)\r\n",
    "ax.legend(loc='best', fontsize=20)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SUMMARY\n",
    "## What we covered\n",
    "<li><b>Dataframes:</b>A reference to a relation in HANA.  No need for deep SQL knowledge.</li>\n",
    "<li><b>HANA ML API:</b>Exploit HANA's ML capabilities using a SciKit type of Python interface.</li>\n",
    "<li><b>Exploratory Data Analysis and Visualization:</b>Analyze large data sets without the performance penalty or running out of resources on the client</li>\n",
    "<br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main benefits\n",
    "<li>Ease of Use: For the data scientists.</li>\n",
    "<li>Performance:  Orders of magnitude performance gains.</li>\n",
    "<li>Security:  Centralized security.</li>\n",
    "<br>\n",
    "<br>"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "interpreter": {
   "hash": "9e8e26eb492012ce43ec3ea98c3fc2503d5495ecd40107dd94395e1e0d860e85"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}