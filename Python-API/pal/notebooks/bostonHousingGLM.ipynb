{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GLM Example\n",
    "Generalized linear models are a generalization of linear regression. In linear regression, responses are modeled as coming from gaussian distributions, each of equal variance, with each distribution's mean given by a linear combination of the predictor variables. GLMs allow other distributions where the variance might vary with the mean, and they allow the mean to be given by a function of the linear predictor instead of taking the linear predictor directly.\n",
    "\n",
    "## Boston housing prices\n",
    "We'll demonstrate by using GLMs to predict the median price of a home in Boston.  The variables are crime rate, zoning information,\n",
    "proportion of non-retail business, etc.  This dataset has median prices in Boston for 1972.  Even though the data is pretty old, the methodology for analytics is valid for more recent datasets.\n",
    "\n",
    "The dataset is from Kaggle. https://www.kaggle.com/c/boston-housing. For tutorials use only.\n",
    "\n",
    "## Housing Values in Suburbs of Boston in 1972\n",
    "\n",
    "The <font color='red'>medv</font> variable is the target variable.\n",
    "### Data description\n",
    "The Boston data frame has 506 rows and 14 columns.\n",
    "This data frame contains the following columns:\n",
    "1. __crim__: per capita crime rate by town.\n",
    "2. __zn__: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. __indus__: proportion of non-retail business acres per town.\n",
    "4. __chas__: Charles River dummy variable (1 if tract bounds river; 0 otherwise).\n",
    "5. __nox__: nitrogen oxides concentration (parts per 10 million).\n",
    "6. __rm__: average number of rooms per dwelling.\n",
    "7. __age__: proportion of owner-occupied units built prior to 1940.\n",
    "8. __dis__: weighted mean of distances to five Boston employment centres.\n",
    "9. __rad__: index of accessibility to radial highways.\n",
    "10. __tax__: full-value property-tax rate per \\$10000\n",
    "11. __ptratio__: pupil-teacher ratio by town.\n",
    "12. __black__: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n",
    "13. __lstat__: lower status of the population (percent).\n",
    "14. __medv__: median value of owner-occupied homes in $1000s.\n",
    "</td></tr></table>\n",
    "\n",
    "### Factoids\n",
    "The prices in Boston across years are below.  If we had a historical dataset, an analysis could be done to account for the macro trends as well.\n",
    "\n",
    "The second graph shows the intuition we have with respect to prices in relation to crime rate.  It is expected that house prices will be lower in areas where crime rates are higher.\n",
    "\n",
    "The third figure is a chart showing how inflation may affect prices.  So, for deeper analysis and prediction, we may want to consider inflation.\n",
    "\n",
    "In this notebook, these factors are not considered.  They are here to demonstrate the need for deep domain analysis.\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"images/boston_prices_by_year.png\" alt=\"Boston home prices\" title=\"Boston housing prices\" style=\"float:left;\" /></td>\n",
    "<td><img src=\"images/Crime-Rate-and-Median-House-Prices.png\" alt=\"Boston home prices\" title=\"Boston housing prices\"  /></td>\n",
    "<td><img src=\"images/Inflation_Adjusted_Housing_Prices_1890_2006.jpg\" alt=\"Inflation adjusted prices\" title=\"Inflation adjusted prices\" style=\"float:left;\" />\n",
    "</td></tr></table>\n",
    "\n",
    "\n",
    "In this notebook, we will use the dataset for Boston housing prices and predict the price based on numerous factors."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml import dataframe\r\n",
    "from hana_ml.algorithms.pal import regression\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import logging"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data\r\n",
    "The data is loaded into 4 tables, for full, training, validation, and test sets:\r\n",
    "<li>BOSTON_HOUSING_PRICES</li>\r\n",
    "<li>BOSTON_HOUSING_PRICES_TRAINING</li>\r\n",
    "<li>BOSTON_HOUSING_PRICES_VALIDATION</li>\r\n",
    "<li>BOSTON_HOUSING_PRICES_TEST</li>\r\n",
    "\r\n",
    "To do that, a connection is created and passed to the loader.\r\n",
    "\r\n",
    "There is a config file, config/e2edata.ini that controls the connection parameters and whether or not to reload the data from scratch.  In case the data is already loaded, there would be no need to load the data.  A sample section is below.  If the config parameter, reload_data is true then the tables for test, training, and validation are (re-)created and data inserted into them.\r\n",
    "\r\n",
    "#########################<br>\r\n",
    "[hana]<br>\r\n",
    "url=host.sjc.sap.corp<br>\r\n",
    "user=username<br>\r\n",
    "passwd=userpassword<br>\r\n",
    "port=3xx15<br>\r\n",
    "<br>\r\n",
    "[bostonhousingdataset]<br>\r\n",
    "reload_data=true\r\n",
    "#########################<br>\r\n",
    "## Define Datasets - Training, validation, and test sets\r\n",
    "Data frames represent data in HANA and HANA-side queries on that data, so computation on large data sets in HANA can happen in HANA.  Trying to bring the entire data set into the client may be impractical or impossible for large data sets.\r\n",
    "\r\n",
    "The original/full dataset is split into training, test and validation sets.  In the example below, they reside in different tables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.algorithms.pal.utility import DataSets, Settings\r\n",
    "url, port, user, pwd = Settings.load_config(\"../../config/e2edata.ini\")\r\n",
    "connection_context = dataframe.ConnectionContext(url, port, user, pwd)\r\n",
    "full_set, training_set, validation_set, test_set = DataSets.load_boston_housing_data(connection_context, force=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Exploration\n",
    "Let us look at the number of rows in the data set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Number of rows in full set: {}'.format(full_set.count()))\r\n",
    "print('Number of rows in training set: {}'.format(training_set.count()))\r\n",
    "print('Number of rows in validation set: {}'.format(validation_set.count()))\r\n",
    "print('Number of rows in test set: {}'.format(test_set.count()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's look at the columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(full_set.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's look at the data types"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "full_set.dtypes()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set up the features and labels for the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'BLACK', 'LSTAT']\r\n",
    "label='MEDV'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create model using training data\n",
    "The first GLM we create will use the default settings of `family='gaussian'` and `link='identity'`. A GLM using these settings is equivalent to linear regression."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "SAP HANA Predictive Analytics Library takes DOUBLE and INTEGER data types for most numeric types.  Since we have DECIMALs and TINYINTs in our data set, we cast them to the types required by PAL."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Cast to correct types so PAL can consume it.\r\n",
    "dfts = training_set.cast(['CRIM', \"ZN\", \"INDUS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"PTRATIO\", \"BLACK\", \"LSTAT\", \"MEDV\"], \"DOUBLE\")\r\n",
    "dfts = dfts.cast([\"CHAS\", \"RAD\", \"TAX\"], \"INTEGER\")\r\n",
    "dfts = dfts.to_head(\"ID\")\r\n",
    "dfts.head(5).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "linear_model = regression.GLM()\r\n",
    "linear_model.fit(dfts, key='ID', features=features, label=label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how well this model does. We'll compute the R^2 score of its predictions on the test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_test = test_set.cast(['CRIM', \"ZN\", \"INDUS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"PTRATIO\", \"BLACK\", \"LSTAT\", \"MEDV\"], \"DOUBLE\")\r\n",
    "df_test = df_test.cast([\"CHAS\", \"RAD\", \"TAX\"], \"INTEGER\")\r\n",
    "df_test = df_test.to_head(\"ID\")\r\n",
    "linear_model.score(df_test, key='ID', features=features, label=label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try a few others. We'll experiment with gamma distributions and inverse or logarithmic link functions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gaussian_idlink = regression.GLM(family='gaussian', link='identity')\r\n",
    "gamma_invlink = regression.GLM(family='gamma', link='inverse')\r\n",
    "gaussian_invlink = regression.GLM(family='gaussian', link='inverse')\r\n",
    "gamma_loglink = regression.GLM(family='gamma', link='log')\r\n",
    "gaussian_loglink = regression.GLM(family='gaussian', link='log')\r\n",
    "for model in [gaussian_idlink, gamma_invlink, gaussian_invlink, gamma_loglink, gaussian_loglink]:\r\n",
    "    model.fit(dfts, key='ID', features=features, label=label)\r\n",
    "    print(model.score(df_test, key='ID', features=features, label=label))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "An inverse link seems to help model our data better. Our error terms seem to be modeled better by our original choice of `family='gaussian'` than `family='gamma'`."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "interpreter": {
   "hash": "9e8e26eb492012ce43ec3ea98c3fc2503d5495ecd40107dd94395e1e0d860e85"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}