{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest Regression Example\n",
    "\n",
    "## Boston housing prices\n",
    "The objective is to predict the median price of a home in Boston.  The variables are crime rate, zoning information,\n",
    "proportion of non-retail business, etc.  This dataset has median prices in Boston for 1972.  Even though the data is pretty old, the methodology for analytics is valid for more recent datasets.\n",
    "\n",
    "<b>The purpose of this demonstration is to show the use of SAP HANA's Predictive Analytics Library to created Random forest model.</b>\n",
    "\n",
    "The dataset is from Kaggle. https://www.kaggle.com/c/boston-housing. For tutorials use only.\n",
    "\n",
    "## Housing Values in Suburbs of Boston in 1972\n",
    "\n",
    "The <font color='red'>medv</font> variable is the target variable.\n",
    "### Data description\n",
    "The Boston data frame has 506 rows and 14 columns.\n",
    "This data frame contains the following columns:\n",
    "1. __crim__: per capita crime rate by town.\n",
    "2. __zn__: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. __indus__: proportion of non-retail business acres per town.\n",
    "4. __chas__: Charles River dummy variable (1 if tract bounds river; 0 otherwise).\n",
    "5. __nox__: nitrogen oxides concentration (parts per 10 million).\n",
    "6. __rm__: average number of rooms per dwelling.\n",
    "7. __age__: proportion of owner-occupied units built prior to 1940.\n",
    "8. __dis__: weighted mean of distances to five Boston employment centres.\n",
    "9. __rad__: index of accessibility to radial highways.\n",
    "10. __tax__: full-value property-tax rate per \\$10000\n",
    "11. __ptratio__: pupil-teacher ratio by town.\n",
    "12. __black__: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n",
    "13. __lstat__: lower status of the population (percent).\n",
    "14. __medv__: median value of owner-occupied homes in $1000s.\n",
    "</td></tr></table>\n",
    "\n",
    "### Factoids\n",
    "The prices in Boston across years is below.  If we had a historical dataset, an analysis could be done to account for the macro trends as well.\n",
    "\n",
    "The second graph shows the intuition we have with respect to prices in relation to crime rate.  It is expected that house prices will be lower in areas where crime rates are higher.\n",
    "\n",
    "The third figure is a chart showing how inflation may affect prices.  So, for deeper analysis and prediction, we may want to consider inflation.\n",
    "\n",
    "In this notebook, these factors are not considered.  They are here to demonstrate the need for deep domain analysis.\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"images/boston_prices_by_year.png\" alt=\"Boston home prices\" title=\"Boston housing prices\" style=\"float:left;\" /></td>\n",
    "<td><img src=\"images/Crime-Rate-and-Median-House-Prices.png\" alt=\"Boston home prices\" title=\"Boston housing prices\"  /></td>\n",
    "<td><img src=\"images/Inflation_Adjusted_Housing_Prices_1890_2006.jpg\" alt=\"Inflation adjusted prices\" title=\"Inflation adjusted prices\" style=\"float:left;\" />\n",
    "</td></tr></table>\n",
    "\n",
    "\n",
    "In this notebook, we will use the dataset for Boston housing prices and predict the price based on numerous factors."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml import dataframe\r\n",
    "from hana_ml.algorithms.pal import clustering\r\n",
    "from hana_ml.algorithms.pal.unified_regression import UnifiedRegression\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import logging"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data\r\n",
    "The data is loaded into 4 tables, for full, training, validation, and test sets:\r\n",
    "<li>BOSTON_HOUSING_PRICES</li>\r\n",
    "<li>BOSTON_HOUSING_PRICES_TRAINING</li>\r\n",
    "<li>BOSTON_HOUSING_PRICES_VALIDATION</li>\r\n",
    "<li>BOSTON_HOUSING_PRICES_TEST</li>\r\n",
    "\r\n",
    "To do that, a connection is created and passed to the loader.\r\n",
    "\r\n",
    "There is a config file, config/e2edata.ini that controls the connection parameters and whether or not to reload the data from scratch.  In case the data is already loaded, there would be no need to load the data.  A sample section is below.  If the config parameter, reload_data is true then the tables for test, training, and validation are (re-)created and data inserted into them.\r\n",
    "\r\n",
    "Although this ini file has other sections, please do not modify them. Only the [hana] section should be modified.\r\n",
    "\r\n",
    "#########################<br>\r\n",
    "[hana]<br>\r\n",
    "url=host.sjc.sap.corp<br>\r\n",
    "user=username<br>\r\n",
    "passwd=userpassword<br>\r\n",
    "port=3xx15<br>\r\n",
    "#########################<br>\r\n",
    "## Define Datasets - Training, validation, and test sets\r\n",
    "Data frames are used keep references to data so computation on large data sets in HANA can happen in HANA.  Trying to bring the entire data set into the client will likely result in out of memory exceptions.\r\n",
    "\r\n",
    "The original/full dataset is split into training, test and validation sets.  In the example below, they reside in different tables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.algorithms.pal.utility import DataSets, Settings\r\n",
    "url, port, user, pwd = Settings.load_config(\"../../config/e2edata.ini\")\r\n",
    "connection_context = dataframe.ConnectionContext(url, port, user, pwd)\r\n",
    "full_set, training_set, validation_set, test_set = DataSets.load_boston_housing_data(connection_context, force=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "connection_context.hana_version()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Exploration\n",
    "Let us look at the number of rows in the data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Number of rows in full set: {}'.format(full_set.count()))\r\n",
    "print('Number of rows in training set: {}'.format(training_set.count()))\r\n",
    "print('Number of rows in validation set: {}'.format(validation_set.count()))\r\n",
    "print('Number of rows in test set: {}'.format(test_set.count()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's look at the columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(full_set.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's look at the data types"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "full_set.dtypes()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set up the features and labels for the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'BLACK', 'LSTAT']\r\n",
    "label='MEDV'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create model using training data\n",
    "For demonstration, we will create two models, model and model_with_id, one where we have a unique id in the training set and one where there is none.\n",
    "\n",
    "We are using Random Forest regression and SVM routines in this example\n",
    "\n",
    "Documentation is <a href=\"https://help.sap.com/http.svc/rc/DRAFT/3f0dbe754b194c42a6bf3405697b711f/2.0.031/en-US/html/index.html\">here</a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "SAP HANA Predictive Analytics Library takes DOUBLE and INTEGER data types for most numeric types.  Since we have DECIMALs and TINYINTs in our data set, we cast them to the types required by PAL."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Cast to correct types so PAL can consume it.\n",
    "dfts = training_set.cast(['CRIM', \"ZN\", \"INDUS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"PTRATIO\", \"BLACK\", \"LSTAT\", \"MEDV\"], \"DOUBLE\")\n",
    "dfts = dfts.cast([\"CHAS\", \"RAD\", \"TAX\"], \"INTEGER\")\n",
    "dfts = dfts.to_head(\"ID\")\n",
    "dfts.head(5).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the model\n",
    "Although we had seen graphically that only a few features had an impact on housing prices, let us use all the features to create a model.  We will then use the model to check for importance of the features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We build the model without IDs.  Project only the features and the label.\n",
    "df = dfts.select(features, label)\n",
    "model = UnifiedRegression(func='RandomForest')\n",
    "model.fit(df, features=features, label=label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SQL statements executed\n",
    "Calling PAL directly would require a number of SQL statements and all that is encapsulated in the Python library functions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predict using test set\n",
    "Let us now do some predictions and see how well the model generalizes.\n",
    "\n",
    "The predict() method always takes a unique identifier to identify the prediction on a specific data row.  This way, the caller (python programmer) can then join with the original data set to get the rest of the values for that unique row.  The test_set has columns of types that PAL does not deal with and therefore the columns are cast to the types that are accepted.\n",
    "\n",
    "In order to look at the predicted value as well as the true value, the name of the unique identifier for rows in the result table is renamed to PREDICTED_ID.  This result table is joined with the test set so the predicted and true value can be compared.\n",
    "\n",
    "For the predictions we look at the standard error.  The standard error is defined as the number of standard deviations away the prediction is from the true value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_test = test_set.cast(['CRIM', \"ZN\", \"INDUS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"PTRATIO\", \"BLACK\", \"LSTAT\", \"MEDV\"], \"DOUBLE\")\n",
    "df_test = df_test.cast([\"CHAS\", \"RAD\", \"TAX\"], \"INTEGER\")\n",
    "df_test = df_test.to_head(\"ID\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Note that we are renaming the column ID in the result of predict()\n",
    "result_df = model.predict(df_test, key= 'ID', features=features).rename_columns({'ID': 'PREDICTED_ID'})\n",
    "# Note the use of join() method to join two tables.\n",
    "jdf = result_df.join(test_set, '{}.\"PREDICTED_ID\"={}.\"ID\"'.format(result_df.name, test_set.name), how='inner')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.visualizers.model_debriefing import TreeModelDebriefing\n",
    "\n",
    "shapley_explainer = TreeModelDebriefing.shapley_explainer(result_df, df_test, key='ID', label='MEDV')\n",
    "shapley_explainer.summary_plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predictions\n",
    "Let us look at the predictions.  The predicted values are in 'SCORE' and the actual values are in 'MEDV'.  So, we just rename the 'SCORE' column to 'PREDICTED'\n",
    "\n",
    "In addition, the column 'CONFIDENCE' is the standard error which is the number of standard deviations away the actual values is from the predicted value.  This column is renamed to 'STANDARD_ERROR'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "jdf.head(5).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scoring\n",
    "We now score the results from are test data.  The scoring function we use is R^2.\n",
    "\n",
    "__In the function below, PAL is not invoked but a query is directly executed against data in HANA__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_, score = model.score(df_test, key='ID', features=features, label=label)\n",
    "score.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "The model is available and can be saved for later predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  The generated model is in the database.\n",
    "print(model.model_[0].collect())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(model.model_[1].collect())\n",
    "print(model.model_[2].collect())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Close the Connection"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "connection_context.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "interpreter": {
   "hash": "9e8e26eb492012ce43ec3ea98c3fc2503d5495ecd40107dd94395e1e0d860e85"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}