{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ShampooSalesTimeSeries\n",
    "\n",
    "## 1. Introduction and algorithm description\n",
    "This notebook uses the shampoo sales dataset to demonstrate the time series algorithms below which are provided by the hana_ml. \n",
    "\n",
    "- ARIMA\n",
    "- Auto ARIMA\n",
    "- Auto Exponential Smoothing\n",
    "- Seasonal Decompose"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### - ARIMA \n",
    "The Auto Regressive Integrated Moving Average (ARIMA) algorithm is famous in econometrics, statistics and time series analysis.\n",
    "There are three integers (p, d, q) that are used to parametrize ARIMA models. Because of that, a nonseasonal ARIMA model is denoted with ARIMA(p, d, q):\n",
    "\n",
    " - p is the number of autoregressive terms (AR part). It allows to incorporate the effect of past values into our model. Intuitively, this would be similar to stating that it is likely to be warm tomorrow if it has been warm the past 3 days.\n",
    " - d is the number of nonseasonal differences needed for stationarity. Intuitively, this would be similar to stating that it is likely to be same temperature tomorrow if the difference in temperature in the last three days has been very small.\n",
    " - q is the number of lagged forecast errors in the prediction equation (MA part). This allows us to set the error of our model as a linear combination of the error values observed at previous time points in the past.\n",
    "\n",
    "When dealing with seasonal effects, Seasonal ARIMA(SARIMA) is used, which is denoted as ARIMA(p,d,q)(P,D,Q,s). Here, p, d, q are the nonseasonal parameters described above, P, D, Q follow the same definition but are applied to the seasonal component of the time series. The term s is the periodicity of the time series."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### - Auto ARIMA \n",
    "Although the ARIMA model is useful and powerful in time series analysis, it is somehow difficult to choose appropriate orders. Hence, auto ARIMA is to determine the orders of an ARIMA model automatically."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### - Auto Exponential Smoothing\n",
    "Auto exponential smoothing is used to calculate optimal parameters of a set of smoothing functions, including Single Exponential Smoothing, Double Exponential Smoothing, and Triple Exponential Smoothing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### - Seasonal Decompose\n",
    "The algorithm is to decompose a time series into three components: seasonal, trend, and random."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Dataset\n",
    "Shampoo sales dataset describes the monthly number of sales of shampoo over a 3 year period.\n",
    "The units are a sales count and there are 36 observations. The original dataset is credited to Makridakis, Wheelwright and Hyndman (1998). We can see that the dataset shows an increasing trend and possibly has a seasonal component. \n",
    "\n",
    "<img src=\"images/Shampoo-Sales.png\" title=\"Temperatures\" width=\"600\" height=\"1200\" />\n",
    "\n",
    "\n",
    "Dataset source: https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv for tutorials use only.\n",
    "\n",
    "### Attribute information\n",
    " - ID: ID\n",
    " - SALES: Monthly sales \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Data Loading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import packages\n",
    "First, import packages needed in the data loading."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml import dataframe\r\n",
    "from hana_ml.algorithms.pal.utility import DataSets, Settings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup Connection\n",
    "In our case, the data is loaded into a table called \"SHAMPOO_SALES_DATA_TBL\" in HANA from a csv file \"shampoo.csv\".\n",
    "To do that, a connection to HANA is created and then passed to the data loader.\n",
    "To create a such connection, a config file, <b>config/e2edata.ini</b> is used to control the connection parameters.\n",
    "A sample section in the config file is shown below which includes HANA url, port, user and password information.  \n",
    "\n",
    "#########################<br>\n",
    "[hana]<br>\n",
    "url=host-url<br>\n",
    "user=username<br>\n",
    "passwd=userpassword<br>\n",
    "port=3xx15<br>\n",
    "#########################<br>\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "url, port, user, pwd = Settings.load_config(\"../../config/e2edata.ini\")\r\n",
    "connection_context = dataframe.ConnectionContext(url, port, user, pwd)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Data\n",
    "Then, the function DataSets.load_shampoo_data() is used to decide load or reload the data from scratch. If it is the first time to load data, an exmaple of return message is shown below:\n",
    "\n",
    "##################<br>\n",
    "ERROR:hana_ml.dataframe:Failed to get row count for the current Dataframe, (259, 'invalid table name:  Could not find table/view SHAMPOO_SALES_DATA_TBL in schema XIN: line 1 col 37 (at pos 36)')\n",
    "Table SHAMPOO_SALES_DATA_TBL doesn't exist in schema XIN\n",
    "Creating table SHAMPOO_SALES_DATA_TBL in schema XIN ....\n",
    "Drop unsuccessful\n",
    "Creating table XIN.SHAMPOO_SALES_DATA_TBL\n",
    "Data Loaded:100%\n",
    "###################<br>\n",
    "\n",
    "If the data is already loaded, there would be a return message \"Table XXX exists and data exists\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = DataSets.load_shampoo_data(connection_context)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple Data Exploration\n",
    "We will do some data exploration to know the data better.\n",
    "- First 3 data points"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.collect().head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(df.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- No. of data points"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Number of rows in df: {}'.format(df.count()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Data types"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.dtypes()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Analysis\n",
    "In this section, various time series algorithms are applied to analyze the shampoo sales dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Seasonal Decompose\n",
    "Because the dataset shows an increasing trend and possibly some seasonal component, we first use seasonal decompose function to decompose the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.algorithms.pal.tsa.seasonal_decompose import seasonal_decompose"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stats, decompose = seasonal_decompose(df, endog= 'SALES', alpha = 0.2, thread_ratio=0.5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "seasonal decompose function returns two tables: stats and decompose."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stats.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could see the data has a seasonality and its period is 2. The corresponding multiplicative seasonality model is identified. The decompose table shows the components."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "decompose.collect().head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 ARIMA\n",
    "import the ARIMA module"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.algorithms.pal.tsa.arima import ARIMA"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create an ARIMA estimator and make the initialization:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "arima = ARIMA(order=(1, 0, 0), seasonal_order=(1, 0, 0, 2),\r\n",
    "              method='mle', thread_ratio=1.0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform fit on the given data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "arima.fit(df, endog='SALES')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are two attributes of ARIMA model: model_ and fitted_. We could see the model parameters in model_. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "arima.model_.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model_ contains AIC (Akaike Information Criterion) and BIC (Bayes Information Criterion) that can be minimized to select the best fitting model. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "arima.fitted_.collect().set_index('ID').head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predict uisng the ARIMA model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = arima.predict(forecast_method='innovations_algorithm',forecast_length=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\r\n",
    "\r\n",
    "from hana_ml.visualizers.visualizer_base import forecast_line_plot\r\n",
    "\r\n",
    "ax = forecast_line_plot(pred_data=result.set_index(\"TIMESTAMP\"),\r\n",
    "                        confidence=(\"LO80\", \"HI80\", \"LO95\", \"HI95\"),\r\n",
    "                        max_xticklabels=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3 Auto ARIMA \n",
    "Import auto ARIMA module"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.algorithms.pal.tsa.auto_arima import AutoARIMA"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create an auto ARIMA estimator and make the initialization:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "autoarima = AutoARIMA(search_strategy=1, allow_linear=1, thread_ratio=1.0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform fit on the given data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "autoarima.fit(df, endog='SALES')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "autoarima.model_.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "autoarima.fitted_.collect().set_index('ID').head(6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predict uisng the auto ARIMA model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result= autoarima.predict(forecast_method='innovations_algorithm', forecast_length=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4 Auto Exponential Smoothing \n",
    "Import auto exponential smoothing module:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.algorithms.pal.tsa.exponential_smoothing import AutoExponentialSmoothing"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create an auto exponential smoothing estimator and make the initialization:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "autoexpsmooth = AutoExponentialSmoothing(model_selection=1, forecast_num=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform the fit on the given data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "autoexpsmooth.fit_predict(df,endog= 'SALES',)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Have a look at the stats_ and it shows the parameters and Triple Exponential SMoothing (TESM) model is selected."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "autoexpsmooth.stats_.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To see the result of smoothing forecast and upper and lower bound in the forecast_:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "autoexpsmooth.forecast_.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Close Connection"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "connection_context.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "interpreter": {
   "hash": "9e8e26eb492012ce43ec3ea98c3fc2503d5495ecd40107dd94395e1e0d860e85"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}