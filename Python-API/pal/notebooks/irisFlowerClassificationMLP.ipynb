{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MultiLayer Perceptron Classification Example\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A data set that identifies different types of Iris's is used to demonstrate the use of multi layer perceptron in SAP HANA.  This data set is also used in a clustering example where the objective was to cluster the flowers into three clusters and the intuition was that the three clusters would correspond to the three types of Iris's in the data set.  Since we know the labels (i.e. the types of Iris's), we can use classification to create a model to predict the type of flower based on features or characteristics that are explained below.\n",
    "\n",
    "## Iris Data Set\n",
    "The data set used is from University of California, Irvine (https://archive.ics.uci.edu/ml/datasets/iris). For tutorials use only.  This data set contains attributes of a plant iris.  There are three species of Iris plants.\n",
    "<table>\n",
    "<tr><td>Iris Setosa</td><td><img src=\"images/Iris_setosa.jpg\" title=\"Iris Sertosa\" style=\"float:left;\" width=\"300\" height=\"50\" /></td>\n",
    "<td>Iris Versicolor</td><td><img src=\"images/Iris_versicolor.jpg\" title=\"Iris Versicolor\" style=\"float:left;\" width=\"300\" height=\"50\" /></td>\n",
    "<td>Iris Virginica</td><td><img src=\"images/Iris_virginica.jpg\" title=\"Iris Virginica\" style=\"float:left;\" width=\"300\" height=\"50\" /></td></tr>\n",
    "</table>\n",
    "\n",
    "The data contains the following attributes for various flowers:\n",
    "<table align=\"left\"><tr><td>\n",
    "<li align=\"top\">sepal length in cm</li>\n",
    "<li align=\"left\">sepal width in cm</li>\n",
    "<li align=\"left\">petal length in cm</li>\n",
    "<li align=\"left\">petal width in cm</li>\n",
    "</td><td><img src=\"images/sepal_petal.jpg\" style=\"float:left;\" width=\"200\" height=\"40\" /></td></tr></table>\n",
    "\n",
    "Although the flower is identified in the data set, we will cluster the data set into 3 clusters since we know there are three different flowers.  The hope is that the cluster will correspond to each of the flowers.\n",
    "\n",
    "A different notebook will use a classification algorithm to predict the type of flower based on the sepal and petal dimensions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\r\n",
    "from hana_ml import dataframe\r\n",
    "from hana_ml.algorithms.pal.neural_network import MLPClassifier, MLPRegressor\r\n",
    "from hana_ml.algorithms.pal import metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data\r\n",
    "The data is loaded into 4 tables - full set, test set, training set, and the validation set:\r\n",
    "<li>IRIS_DATA_FULL_TBL</li>\r\n",
    "<li>IRIS_DATA_TRAIN_TBL</li>\r\n",
    "<li>IRIS_DATA_TEST_TBL</li>\r\n",
    "<li>IRIS_DATA_VALIDATION_TBL</li>\r\n",
    "\r\n",
    "To do that, a connection is created and passed to the loader.\r\n",
    "\r\n",
    "There is a config file, <b>config/e2edata.ini</b> that controls the connection parameters and whether or not to reload the data from scratch.  In case the data is already loaded, there would be no need to load the data.  A sample section is below.  If the config parameter, reload_data is true then the tables for test, training, and validation are (re-)created and data inserted into them.\r\n",
    "\r\n",
    "#########################<br>\r\n",
    "[hana]<br>\r\n",
    "url=host.sjc.sap.corp<br>\r\n",
    "user=username<br>\r\n",
    "passwd=userpassword<br>\r\n",
    "port=3xx15<br>\r\n",
    "<br>\r\n",
    "\r\n",
    "#########################<br>\r\n",
    "## Define Datasets - Training, validation, and test sets\r\n",
    "Data frames are used keep references to data so computation on large data sets in HANA can happen in HANA.  Trying to bring the entire data set into the client will likely result in out of memory exceptions.\r\n",
    "\r\n",
    "The original/full dataset is split into training, test and validation sets.  In the example below, they reside in different tables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.algorithms.pal.utility import DataSets, Settings\r\n",
    "import plotting_utils\r\n",
    "url, port, user, pwd = Settings.load_config(\"../../config/e2edata.ini\")\r\n",
    "connection_context = dataframe.ConnectionContext(url, port, user, pwd)\r\n",
    "full_set, training_set, validation_set, test_set = DataSets.load_iris_data(connection_context)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Exploration\n",
    "Let us look at the number of rows in the data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Number of rows in full set: {}'.format(full_set.count()))\r\n",
    "print('Number of rows in training set: {}'.format(training_set.count()))\r\n",
    "print('Number of rows in validation set: {}'.format(validation_set.count()))\r\n",
    "print('Number of rows in test set: {}'.format(test_set.count()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's look at the columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(full_set.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let us look at some rows"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "full_set.head(5).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's look at the data types"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "full_set.dtypes()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's check how many SPECIES are in the data set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "full_set.distinct(\"SPECIES\").collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Model\n",
    "The lines below show the ease with which clustering can be done."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set up the features and labels for the model and create the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\r\n",
    "from plotting_utils import DrawNN\r\n",
    "features = ['SEPALLENGTHCM','SEPALWIDTHCM','PETALLENGTHCM','PETALWIDTHCM']\r\n",
    "label = 'SPECIES'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural Network Architecture"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "network = DrawNN( [1, 10, 1] )\r\n",
    "network.draw(False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Creation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlpc = MLPClassifier(hidden_layer_size=(10,), activation='TANH', output_activation='TANH',\r\n",
    "                     training_style='batch', max_iter=100, normalization='z-transform',\r\n",
    "                     weight_init='uniform', thread_ratio=1)\r\n",
    "mlpc.fit(training_set, 'ID', features, label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Storage"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.model_storage import ModelStorage\r\n",
    "model_storage = ModelStorage(connection_context)\r\n",
    "\r\n",
    "mlpc.name = 'MLPC'  # The model name is mandatory\r\n",
    "mlpc.version = 1\r\n",
    "model_storage.save_model(model=mlpc)\r\n",
    "#need to increase version\r\n",
    "\r\n",
    "# Lists models\r\n",
    "model_storage.list_models()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_storage.list_models()['JSON'].iloc[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = model_storage.load_model(name='MLPC', version=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.predict(data=test_set, key='ID', features=features)[0].collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model can be deleted in SAP HANA DB accoridng to model name and version:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_storage.delete_model('MLPC', 1)\r\n",
    "model_storage.list_models()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Accuracy\n",
    "Let us compute the accuracy on our training and test sets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy = mlpc.score(training_set, 'ID', features, label)\r\n",
    "print(\"Training set accuracy: %f\" % accuracy)\r\n",
    "print(\"Test set accuracy: %f\" % mlpc.score(test_set, 'ID', features, label))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Precision, Recall, Confusion Matrix\n",
    "Accuracy is usually not a good metric to evaluate a model.  Above, we see that we do pretty well for both the training and test sets.\n",
    "Let us look at another metric.\n",
    "\n",
    "To do that we first inspect the results of our test_set predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions_df, soft_max_df = mlpc.predict(test_set, 'ID', features)\r\n",
    "print(soft_max_df.head(5).collect())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function to get the confusion matrix takes in a single data frame with the true label and the predicted label.\n",
    "So, let us construct this data frame by joining on the ID column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ts = test_set.rename_columns({'ID': 'TID'}) #.cast('SPECIES', 'NVARCHAR(256)')\r\n",
    "jsql = '{}.\"{}\"={}.\"{}\"'.format(predictions_df.quoted_name, 'ID', ts.quoted_name, 'TID')\r\n",
    "results_df = predictions_df.join(ts, jsql, how='inner')\r\n",
    "cm_df, classification_report_df = metrics.confusion_matrix(results_df, key='ID', label_true='SPECIES', label_pred='TARGET') "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Confusion Matrix\")\r\n",
    "cm_df.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "from hana_ml.visualizers.metrics import MetricsVisualizer\r\n",
    "f, ax1 = plt.subplots(1,1)\r\n",
    "mv1 = MetricsVisualizer(ax1)\r\n",
    "ax1 = mv1.plot_confusion_matrix(cm_df, normalize=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Recall, precision, and F-measures\")\r\n",
    "classification_report_df.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "connection_context.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "interpreter": {
   "hash": "9e8e26eb492012ce43ec3ea98c3fc2503d5495ecd40107dd94395e1e0d860e85"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}