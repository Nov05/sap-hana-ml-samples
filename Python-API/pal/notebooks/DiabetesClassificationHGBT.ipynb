{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hybrid Gradient Boosting Trees Classification Example \n",
    "\n",
    "A data set that identifies whether or not a pentient has diabetes is used to demonstrate the use of hybrid graident boosting classifier in SAP HANA.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pima Indians Diabetes Dataset\n",
    "\n",
    "Original data comes from National Institute of Diabetes and Digestive and Kidney Diseases. The collected dataset is aiming at, based on certain diagnostic measurements, diagnostically predicting whether or not a patient has diabetes. In particular, patients contained in the dataset are females of Pima Indian heritage, all above the age of 20. Dataset is form Kaggle, for tutorials use only.\n",
    "\n",
    "The dataset contains the following diagnositic <b>attributes</b>:<br>\n",
    "$\\rhd$ \"PREGNANCIES\" - Number of times pregnant,<br>\n",
    "$\\rhd$ \"GLUCOSE\" - Plasma glucose concentration a 2 hours in an oral glucose tolerance test,<br>\n",
    "$\\rhd$ \"BLOODPRESSURE\" -  Diastolic blood pressure (mm Hg),<br>\n",
    "$\\rhd$ \"SKINTHICKNESS\" -  Triceps skin fold thickness (mm),<br>\n",
    "$\\rhd$ \"INSULIN\" - 2-Hour serum insulin (mu U/ml),<br>\n",
    "$\\rhd$ \"BMI\" - Body mass index $(\\text{weight in kg})/(\\text{height in m})^2$,<br>\n",
    "$\\rhd$ \"PEDIGREE\" - Diabetes pedigree function,<br>\n",
    "$\\rhd$ \"AGE\" -  Age (years),<br>\n",
    "$\\rhd$ \"CLASS\" - Class variable (0 or 1) 268 of 768 are 1(diabetes), the others are 0(non-diabetes).\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import hana_ml\r\n",
    "from hana_ml import dataframe\r\n",
    "from hana_ml.algorithms.pal import metrics\r\n",
    "from hana_ml.algorithms.pal.trees import HybridGradientBoostingClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data\r\n",
    "\r\n",
    "The data is loaded into 3 tables - full set, training-validation set, and test set as follows:\r\n",
    "\r\n",
    "<li> PIMA_INDIANS_DIABETES_TBL</li>\r\n",
    "<li> PIMA_INDIANS_DIABETES_TRAIN_VALID_TBL</li>\r\n",
    "<li> PIMA_INDIANS_DIABETES_TEST_TBL</li>\r\n",
    "\r\n",
    "To do that, a connection is created and passed to the loader.\r\n",
    "\r\n",
    "There is a config file, <b>config/e2edata.ini</b> that controls the connection parameters and whether or not to reload the data from scratch. In case the data is already loaded, there would be no need to load the data. A sample section is below. If the config parameter, reload_data is true then the tables for test, training and validation are (re-)created and data inserted into them.\r\n",
    "\r\n",
    "#########################<br>\r\n",
    "[hana]<br>\r\n",
    "url=host.sjc.sap.corp<br>\r\n",
    "user=username<br>\r\n",
    "passwd=userpassword<br>\r\n",
    "port=3xx15<br>\r\n",
    "#########################<br>\r\n",
    "# Define Datasets - training, validation and test sets\r\n",
    "\r\n",
    "Data frames are used keep references to data so computation on large data sets in HANA can happen in HANA. Trying to bring the entire data set into the client will likely result in out of memory exceptions.\r\n",
    "\r\n",
    "The original/full dataset is split into training, test and validation sets. In the example below, they reside in different tables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.algorithms.pal.utility import DataSets, Settings\r\n",
    "import plotting_utils\r\n",
    "url, port, user, pwd = Settings.load_config(\"../../config/e2edata.ini\")\r\n",
    "connection_context = dataframe.ConnectionContext(url, port, user, pwd)\r\n",
    "diabetes_full, diabetes_train_valid, diabetes_test, _ = DataSets.load_diabetes_data(connection_context)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple Exploration\n",
    "\n",
    "Let us look at the number of rows in each dataset:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Number of rows in full set: {}'.format(diabetes_full.count()))\r\n",
    "print('Number of rows in training-validation set: {}'.format(diabetes_train_valid.count()))\r\n",
    "print('Number of rows in test set: {}'.format(diabetes_test.count()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us look at columns of the dataset:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(diabetes_full.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us also look some (in this example, the top 6) rows of the dataset:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "diabetes_full.head(6).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also check the data type of all columns:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "diabetes_full.dtypes()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have a 'CLASS' column in the dataset, let us check how many classes are contained in this dataset:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "diabetes_full.distinct('CLASS').collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Two classes are available, assuring that this is a binary classification problem."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(diabetes_test.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Model Creation & Model Selection\n",
    "The lines below show the ease with which classification can be done.\n",
    "\n",
    "Set up the label column, use default feature set and create the model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cv_range = []\r\n",
    "cv_range.append(('learning_rate',[0.1, 1.0, 3]))\r\n",
    "cv_range.append(('n_estimators', [4, 10, 3]))\r\n",
    "cv_range.append(('split_threshold', [0.1, 1.0, 3]))\r\n",
    "hgc = HybridGradientBoostingClassifier(n_estimators=4, split_threshold=0,\r\n",
    "                                       learning_rate=0.5, fold_num=5, max_depth=6,\r\n",
    "                                       resampling_method='cv', cross_validation_range=cv_range,\r\n",
    "                                       evaluation_metric='error_rate')\r\n",
    "hgc.fit(diabetes_train_valid, key= 'ID', label='CLASS', categorical_variable=['CLASS'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.algorithms.pal.model_selection import GridSearchCV\r\n",
    "from hana_ml.algorithms.pal.model_selection import RandomSearchCV\r\n",
    "hgc2 = HybridGradientBoostingClassifier(max_depth=6)\r\n",
    "\r\n",
    "gscv = GridSearchCV(estimator=hgc2, \r\n",
    "                    param_grid={'learning_rate': [0.1, 0.4, 0.7, 1],\r\n",
    "                                'n_estimators': [4, 6, 8, 10],\r\n",
    "                                'split_threshold': [0.1, 0.4, 0.7, 1]},\r\n",
    "                    train_control={\"fold_num\": 5, \"resampling_method\": 'cv'}, scoring='error_rate')\r\n",
    "gscv.fit(data=diabetes_train_valid, key= 'ID', label='CLASS', categorical_variable=['CLASS'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "\n",
    "Let us compare cross-validation accuracy and test accuracy:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cm = hgc.confusion_matrix_.collect()\r\n",
    "cm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gscv.estimator.confusion_matrix_.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_accuracy = float(cm['COUNT'][cm['ACTUAL_CLASS']==cm['PREDICTED_CLASS']].sum())/cm['COUNT'].sum()\r\n",
    "train_accuracy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features = diabetes_full.columns\r\n",
    "features.remove('CLASS')\r\n",
    "features.remove('ID')\r\n",
    "print(features)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred_res = hgc.predict(diabetes_test, key='ID', features=features)\r\n",
    "pred_res.head(10).collect()\r\n",
    "pred_res.dtypes()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ts = diabetes_test.rename_columns({'ID': 'TID'}) .cast('CLASS', 'NVARCHAR(256)')\r\n",
    "jsql = '{}.\"{}\"={}.\"{}\"'.format(pred_res.quoted_name, 'ID', ts.quoted_name, 'TID')\r\n",
    "results_df = pred_res.join(ts, jsql, how='inner')\r\n",
    "cm_df, classification_report_df = metrics.confusion_matrix(results_df, key='ID', label_true='CLASS', label_pred='SCORE') "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "from hana_ml.visualizers.metrics import MetricsVisualizer\r\n",
    "f, ax1 = plt.subplots(1,1)\r\n",
    "mv1 = MetricsVisualizer(ax1)\r\n",
    "ax1 = mv1.plot_confusion_matrix(cm_df, normalize=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Recall, Precision and F_measures.\")\n",
    "classification_report_df.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from hana_ml.model_storage import ModelStorage\n",
    "from hana_ml.model_storage_services import ModelSavingServices\n",
    "\n",
    "\n",
    "# Creates an object model_storage\n",
    "\n",
    "# model storage must use the same connection than the model\n",
    "model_storage = ModelStorage(connection_context=connection_context)\n",
    "\n",
    "# Saves the model\n",
    "gscv.estimator.name = 'Model A'  # The model name is mandatory\n",
    "gscv.estimator.version = 1\n",
    "model_storage.save_model(model=gscv.estimator)\n",
    "#need to increase version\n",
    "\n",
    "# Lists models\n",
    "model_storage.list_models()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "interpreter": {
   "hash": "9e8e26eb492012ce43ec3ea98c3fc2503d5495ecd40107dd94395e1e0d860e85"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}