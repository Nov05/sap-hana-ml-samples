{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo - SAP HANA Cloud Machine Learning\n",
    "\n",
    " SAP HANA CLoud openSAP course, Week4 / Unit4: Machine Learning\n",
    " \n",
    " Demo scenario: classify peoples likelyhood for acquiring diabetes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the SAP HANA Cloud Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.21121103\n"
     ]
    }
   ],
   "source": [
    "#Import HANA database client library for Python\n",
    "import hdbcli\n",
    "from hdbcli import dbapi\n",
    "#print(hdbcli.__version__)\n",
    "\n",
    "#Import hana_ml package withHANA Dataframe and PAL algorithm classes\n",
    "import hana_ml\n",
    "print(hana_ml.__version__)\n",
    "\n",
    "# Load Dataframe and Connection classes\n",
    "from hana_ml import dataframe\n",
    "from hana_ml.dataframe import ConnectionContext\n",
    "\n",
    "# Load algorithm classes\n",
    "from hana_ml.algorithms import pal\n",
    "from hana_ml.algorithms import apl\n",
    "from hana_ml.algorithms.pal.unified_classification import UnifiedClassification\n",
    "from hana_ml.algorithms.pal.unified_regression import UnifiedRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to SAP HANA Cloud instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conn = dataframe.ConnectionContext( address=\"<hana-system>\", port=<SQL-port>,  user=\"<HANA-user>\", password=\"<password>\")\n",
    "hc_url = '<hanacloud SQL endpoint URL>'\n",
    "\n",
    "conn = dataframe.ConnectionContext( address=hc_url, port=443, user=\"<user>\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.hana_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create HANA dataframe and explore the data from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe using SAP HANA Cloud table specification\n",
    "diabetes_hdf = conn.table(\"DIABETES_DATA\", schema=\"MLLAB_SHARE\")\n",
    "print(diabetes_hdf.select_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe Overview in SAP HANA Cloud\n",
    "print('Number of records', diabetes_hdf.count())\n",
    "print('Number of columns', len(diabetes_hdf.columns))\n",
    "print(diabetes_hdf.columns)\n",
    "print(diabetes_hdf.dtypes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dataframe, move ID as first column, cast target column as categorial\n",
    "diabetes_hdf = diabetes_hdf.to_head('ID')\n",
    "diabetes_hdf = diabetes_hdf.cast('CLASS', 'NVARCHAR(10)')\n",
    "print(diabetes_hdf.select_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at the acual data. Collect() will transfer data from the HANA to the python client\n",
    "# Note, here head(6) will filter TOP 6 rows before the collect()-transfer to Python\n",
    "diabetes_hdf.head(6).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show column descriptive statistics using the describe method\n",
    "diabetes_hdf.describe().head(10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the target variable 'CLASS' and its value distribution (aggregate-count())\n",
    "diabetes_hdf.agg([('count', 'ID', 'N')], group_by='CLASS').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the input data set\n",
    "# default: training_percentage = 0.8, testing_percentage = 0.1, validation_percentage = 0.1\n",
    "\n",
    "from hana_ml.algorithms.pal.partition import train_test_val_split as split\n",
    "d_train, d_test, d_val = split( data= diabetes_hdf, partition_method='stratified', stratified_column='CLASS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows in training subset: {}'.format(d_train.count()))\n",
    "print('Number of rows in validation subset: {}'.format(d_val.count()))\n",
    "print('Number of rows in test subset: {}'.format(d_test.count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train a SAP HANA PAL Hybrid Gradient Boosting classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use UnifiedClassification PAL procedure interface in Python\n",
    "from hana_ml.algorithms.pal.unified_classification import UnifiedClassification\n",
    "\n",
    "# Iterate over different Paremeter Settings of the Algorithm, find best setting\n",
    "from hana_ml.algorithms.pal.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HGBT_MODEL = UnifiedClassification('HybridGradientBoostingTree')\n",
    "\n",
    "MODEL_SEARCH = GridSearchCV(estimator=HGBT_MODEL, \n",
    "                    param_grid={'learning_rate': [0.1, 0.4, 0.7, 1],\n",
    "                                'n_estimators': [4, 6, 8, 10],\n",
    "                                'split_threshold': [0.1, 0.4, 0.7, 1]},\n",
    "                    train_control=dict(fold_num=5,\n",
    "                                       resampling_method='cv',\n",
    "                                       random_state=1,\n",
    "                                       ref_metric=['auc']),\n",
    "                    scoring='error_rate')\n",
    "\n",
    "MODEL_SEARCH.fit(data=d_train, key= 'ID',\n",
    "         label='CLASS',\n",
    "         partition_method='stratified',\n",
    "         partition_random_state=1,\n",
    "         stratified_column='CLASS',\n",
    "         build_report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HGBT_MODEL.confusion_matrix_.collect()\n",
    "#HGBT_MODEL.statistics_.collect()\n",
    "#HGBT_MODEL.metrics_.collect()\n",
    "HGBT_MODEL.optimal_param_.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HGBT_MODEL.importance_.sort('IMPORTANCE', desc=True).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_res = (HGBT_MODEL.score(d_test, key='ID', max_result_num=10, ntiles=20)[1])\n",
    "score_res.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature list for Prediction with test data\n",
    "features = d_test.columns\n",
    "features.remove('CLASS')\n",
    "features.remove('ID')\n",
    "\n",
    "pred_res = HGBT_MODEL.predict(d_test, key='ID', features=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_res.head(5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_res.select('ID', 'SCORE', 'CONFIDENCE', 'REASON_CODE', \n",
    "                ('json_query(\"REASON_CODE\", \\'$[0].attr\\')', 'Top1'), \n",
    "                ('json_query(\"REASON_CODE\", \\'$[0].pct\\')', 'PCT_1'), \n",
    "                ('json_query(\"REASON_CODE\", \\'$[1].attr\\')', 'Top2'), \n",
    "                ('json_query(\"REASON_CODE\", \\'$[1].pct\\')', 'PCT_2') ).head(5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(HGBT_MODEL.get_predict_execute_statement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end of Demo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Demo Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download file from https://www.kaggle.com/uciml/pima-indians-diabetes-database\n",
    "# dataset under CCO public domain license https://creativecommons.org/publicdomain/zero/1.0/\n",
    "import pandas as pd\n",
    "df = dataframe.create_dataframe_from_pandas(conn,\n",
    "                                            pd.read_csv(\"../datasets/diabetes.csv\"),\n",
    "                                            table_name=\"DIABETES_DATA\",\n",
    "                                            force=True)\n",
    "df.collect()\n",
    "#.deselect(\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Confusion Matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from hana_ml.visualizers.metrics import MetricsVisualizer\n",
    "f, ax1 = plt.subplots(1,1)\n",
    "mv1 = MetricsVisualizer(ax1)\n",
    "ax1 = mv1.plot_confusion_matrix(HGBT_MODEL.confusion_matrix_, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature list for Prediction with test data\n",
    "features = d_train.columns\n",
    "features.remove('CLASS')\n",
    "features.remove('ID')\n",
    "\n",
    "pred_res = HGBT_MODEL.predict(d_test, key='ID', features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_res.head(5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_res.select('ID', 'SCORE', 'CONFIDENCE', 'REASON_CODE', ('json_query(\"REASON_CODE\", \\'$[0].attr\\')', 'Top1'), ('json_query(\"REASON_CODE\", \\'$[0].pct\\')', 'PCT_1') ).head(2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Beeswarm Shapley Explainer Plot for Test Data\n",
    "import pydotplus\n",
    "import graphviz\n",
    "from hana_ml.visualizers.model_debriefing import TreeModelDebriefing\n",
    "\n",
    "shapley_explainer = TreeModelDebriefing.shapley_explainer(pred_res, d_test, key='ID', label='CLASS')\n",
    "shapley_explainer.summary_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model SQL Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python SQL generation with release 2.11\n",
    "#print(HGBT_MODEL.get_pal_function())\n",
    "#print(HGBT_MODEL.get_fit_parameters())\n",
    "#print(HGBT_MODEL.get_fit_output_table_names())\n",
    "#print(HGBT_MODEL.fit_hdbprocedure)\n",
    "#print(HGBT_MODEL.get_predict_execute_statement())\n",
    "print(HGBT_MODEL.get_fit_execute_statement())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Models and Model Quality Information to MLLAB-Sandbox\n",
    "from hana_ml.model_storage import ModelStorage\n",
    "\n",
    "MLLAB_models = ModelStorage(connection_context=conn)\n",
    "\n",
    "MODEL_SEARCH.estimator.name = 'HGBT DIABETES Classification Model' \n",
    "MODEL_SEARCH.estimator.version = 1\n",
    "MLLAB_models.save_model(model=HGBT_MODEL)\n",
    "# or MLLAB_models.save_model(model=MODEL_SEARCH.estimator)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models = MLLAB_models.list_models()\n",
    "print(list_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CleanUp\n",
    "\n",
    "#MLLAB_models.delete_models(name='HGBT DIABETES MODEL')\n",
    "#MLLAB_models.clean_up()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.visualizers.unified_report import UnifiedReport\n",
    "# Get Model version 1\n",
    "mymodel = MLLAB_models.load_model('HGBT DIABETES Classification Model', 1)\n",
    "\n",
    "UnifiedReport(mymodel).build().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hana_ml-prod",
   "language": "python",
   "name": "hana_ml-prod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
